{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ LangGraph QuickStart íŠœí† ë¦¬ì–¼\n",
    "\n",
    "## ğŸ“š ê°œìš”\n",
    "\n",
    "ì´ íŠœí† ë¦¬ì–¼ì€ LangGraphì˜ í•µì‹¬ ê°œë…ê³¼ ê¸°ëŠ¥ì„ ë‹¨ê³„ë³„ë¡œ í•™ìŠµí•˜ëŠ” ì¢…í•© ê°€ì´ë“œì…ë‹ˆë‹¤. ê¸°ë³¸ ì±—ë´‡ êµ¬ì¶•ë¶€í„° ì‹œì‘í•˜ì—¬ ë„êµ¬ í†µí•©, ë©”ëª¨ë¦¬ ê´€ë¦¬, Human-in-the-Loop, ìƒíƒœ ì»¤ìŠ¤í„°ë§ˆì´ì§•, íƒ€ì„ íŠ¸ë˜ë¸”ê¹Œì§€ LangGraphì˜ ê°•ë ¥í•œ ê¸°ëŠ¥ë“¤ì„ ì²´ê³„ì ìœ¼ë¡œ ìµí ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "## ğŸ¯ í•™ìŠµ ëª©í‘œ\n",
    "\n",
    "1. **ê¸°ë³¸ ì±—ë´‡ êµ¬ì¶•**: StateGraphë¥¼ ì‚¬ìš©í•œ ê¸°ë³¸ì ì¸ ëŒ€í™”í˜• AI êµ¬í˜„\n",
    "2. **ë„êµ¬ í†µí•©**: ì™¸ë¶€ APIì™€ ë„êµ¬ë¥¼ í™œìš©í•œ ì±—ë´‡ ê¸°ëŠ¥ í™•ì¥\n",
    "3. **ë©”ëª¨ë¦¬ ê´€ë¦¬**: ì²´í¬í¬ì¸íŒ…ì„ í†µí•œ ëŒ€í™” ìƒíƒœ ìœ ì§€\n",
    "4. **Human-in-the-Loop**: ì¸ê°„ ê°œì…ì´ í•„ìš”í•œ ì›Œí¬í”Œë¡œìš° êµ¬í˜„\n",
    "5. **ìƒíƒœ ì»¤ìŠ¤í„°ë§ˆì´ì§•**: ë³µì¡í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì„ ìœ„í•œ ìƒíƒœ í™•ì¥\n",
    "6. **íƒ€ì„ íŠ¸ë˜ë¸”**: ì´ì „ ìƒíƒœë¡œ ë˜ëŒì•„ê°€ê¸° ë° ëŒ€ì²´ ê²½ë¡œ íƒìƒ‰\n",
    "\n",
    "## ğŸ”‘ ì£¼ìš” ê°œë…\n",
    "\n",
    "- **StateGraph**: ìƒíƒœ ê¸°ê³„ë¡œ ì±—ë´‡ êµ¬ì¡°ë¥¼ ì •ì˜\n",
    "- **Node**: ì‘ì—… ë‹¨ìœ„ (í•¨ìˆ˜)\n",
    "- **Edge**: ë…¸ë“œ ê°„ ì „í™˜\n",
    "- **State**: ê·¸ë˜í”„ ì „ì²´ì—ì„œ ê³µìœ ë˜ëŠ” ë°ì´í„°\n",
    "- **Checkpointing**: ìƒíƒœ ì €ì¥ ë° ë³µì›\n",
    "- **Tool**: ì™¸ë¶€ ê¸°ëŠ¥ í†µí•©\n",
    "- **Human-in-the-Loop**: ì¸ê°„ ê²€í† /ìŠ¹ì¸ í”„ë¡œì„¸ìŠ¤\n",
    "\n",
    "## ğŸ“‹ ì‚¬ì „ ì¤€ë¹„ì‚¬í•­\n",
    "\n",
    "- Python 3.11 ì´ìƒ\n",
    "- OpenAI API í‚¤\n",
    "- Tavily Search API í‚¤ (Part 2ì—ì„œ í•„ìš”)\n",
    "- LangSmith API í‚¤ (ì„ íƒì‚¬í•­, ì¶”ì ìš©)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í™˜ê²½ ì„¤ì •\n",
    "\n",
    "ë¨¼ì € í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ê³  í™˜ê²½ì„ ì„¤ì •í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API KEYë¥¼ í™˜ê²½ë³€ìˆ˜ë¡œ ê´€ë¦¬í•˜ê¸° ìœ„í•œ ì„¤ì • íŒŒì¼\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY ì •ë³´ë¡œë“œ\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangSmith ì¶”ì ì„ ì„¤ì •í•©ë‹ˆë‹¤. https://smith.langchain.com\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ì´ë¦„ì„ ì…ë ¥í•©ë‹ˆë‹¤.\n",
    "logging.langsmith(\"LangGraph-Tutorial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: ê¸°ë³¸ ì±—ë´‡ êµ¬ì¶• ğŸ¤–\n",
    "\n",
    "ì´ ì„¹ì…˜ì—ì„œëŠ” LangGraphë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ë³¸ì ì¸ ì±—ë´‡ì„ êµ¬ì¶•í•©ë‹ˆë‹¤. ì´ ì±—ë´‡ì€ ì´í›„ íŠœí† ë¦¬ì–¼ì—ì„œ ì ì§„ì ìœ¼ë¡œ ë” ì •êµí•œ ê¸°ëŠ¥ì„ ì¶”ê°€í•  ê¸°ë°˜ì´ ë©ë‹ˆë‹¤.\n",
    "\n",
    "## í•µì‹¬ ê°œë…\n",
    "\n",
    "### StateGraph\n",
    "- ìƒíƒœ(State)ë¡œ ì±—ë´‡ì˜ êµ¬ì¡°ë¥¼ ì •ì˜\n",
    "- **ë…¸ë“œ(Nodes)**: LLMì„ í˜¸ì¶œí•˜ê±°ë‚˜ ê¸°ëŠ¥ì„ ì‹¤í–‰í•˜ëŠ” ë‹¨ìœ„\n",
    "- **ì—£ì§€(Edges)**: ë…¸ë“œ ê°„ ì „í™˜ ë°©ë²•ì„ ì§€ì •\n",
    "\n",
    "### State\n",
    "- ê·¸ë˜í”„ì˜ ìŠ¤í‚¤ë§ˆì™€ ë¦¬ë“€ì„œ í•¨ìˆ˜ë¥¼ í¬í•¨\n",
    "- ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ë¥¼ ê´€ë¦¬í•˜ë©°, ìƒˆ ë©”ì‹œì§€ëŠ” ê¸°ì¡´ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ë¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "# State ì •ì˜: ì±—ë´‡ì˜ ìƒíƒœë¥¼ ë‚˜íƒ€ë‚´ëŠ” íƒ€ì…\n",
    "class State(TypedDict):\n",
    "    \"\"\"ì±—ë´‡ì˜ ìƒíƒœë¥¼ ì •ì˜í•˜ëŠ” íƒ€ì…\n",
    "\n",
    "    messages: ëŒ€í™” ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸\n",
    "    - add_messages í•¨ìˆ˜ë¥¼ í†µí•´ ìƒˆ ë©”ì‹œì§€ê°€ ì¶”ê°€ë¨ (ë®ì–´ì“°ê¸°ê°€ ì•„ë‹Œ ì¶”ê°€)\n",
    "    \"\"\"\n",
    "\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "# StateGraph ìƒì„±\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "print(\"âœ… StateGraph ìƒì„± ì™„ë£Œ!\")\n",
    "print(\"ğŸ“Œ StateëŠ” messages í‚¤ë¥¼ ê°€ì§€ë©°, add_messages ë¦¬ë“€ì„œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM ì„ íƒ ë° ì„¤ì •\n",
    "\n",
    "ì±—ë´‡ì˜ ë‘ë‡Œ ì—­í• ì„ í•  ì–¸ì–´ ëª¨ë¸ì„ ì„ íƒí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM ì„ íƒ\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# OpenAI ëª¨ë¸ ì‚¬ìš©\n",
    "llm = ChatOpenAI(model=\"gpt-4.1\", temperature=0)\n",
    "\n",
    "print(f\"âœ… LLM ì„¤ì • ì™„ë£Œ: {llm.model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì±—ë´‡ ë…¸ë“œ ì¶”ê°€\n",
    "\n",
    "**ë…¸ë“œ(Node)** ëŠ” ì‘ì—… ë‹¨ìœ„ì´ë©°, ì¼ë°˜ì ìœ¼ë¡œ í•¨ìˆ˜ë¡œ êµ¬í˜„ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(state: State):\n",
    "    \"\"\"ì±—ë´‡ ë…¸ë“œ í•¨ìˆ˜\n",
    "\n",
    "    í˜„ì¬ ìƒíƒœì˜ ë©”ì‹œì§€ë¥¼ ë°›ì•„ LLMì— ì „ë‹¬í•˜ê³ ,\n",
    "    ì‘ë‹µì„ ìƒˆ ë©”ì‹œì§€ë¡œ ì¶”ê°€í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    # LLMì„ í˜¸ì¶œí•˜ì—¬ ì‘ë‹µ ìƒì„±\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "\n",
    "    # ì‘ë‹µì„ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•˜ì—¬ ë°˜í™˜\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# ê·¸ë˜í”„ì— ë…¸ë“œ ì¶”ê°€\n",
    "# ì²« ë²ˆì§¸ ì¸ì: ë…¸ë“œì˜ ê³ ìœ  ì´ë¦„\n",
    "# ë‘ ë²ˆì§¸ ì¸ì: ë…¸ë“œê°€ ì‚¬ìš©ë  ë•Œ í˜¸ì¶œë  í•¨ìˆ˜\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "print(\"âœ… ì±—ë´‡ ë…¸ë“œ ì¶”ê°€ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì§„ì…ì (Entry Point)ê³¼ ì¢…ë£Œì (Exit Point) ì¶”ê°€\n",
    "\n",
    "ê·¸ë˜í”„ê°€ ì–´ë””ì„œ ì‹œì‘í•˜ê³  ëë‚˜ëŠ”ì§€ ì •ì˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì§„ì…ì : ê·¸ë˜í”„ ì‹¤í–‰ì´ ì‹œì‘ë˜ëŠ” ì§€ì \n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "\n",
    "# ì¢…ë£Œì : ê·¸ë˜í”„ ì‹¤í–‰ì´ ëë‚˜ëŠ” ì§€ì \n",
    "graph_builder.add_edge(\"chatbot\", END)\n",
    "\n",
    "print(\"âœ… ì§„ì…ì ê³¼ ì¢…ë£Œì  ì„¤ì • ì™„ë£Œ!\")\n",
    "print(\"ğŸ“Œ ì‹¤í–‰ íë¦„: START â†’ chatbot â†’ END\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê·¸ë˜í”„ ì»´íŒŒì¼\n",
    "\n",
    "ê·¸ë˜í”„ë¥¼ ì‹¤í–‰í•˜ê¸° ì „ì— ì»´íŒŒì¼í•´ì•¼ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê·¸ë˜í”„ ì»´íŒŒì¼\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "print(\"âœ… ê·¸ë˜í”„ ì»´íŒŒì¼ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê·¸ë˜í”„ ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "# ê·¸ë˜í”„ ì‹œê°í™”\n",
    "visualize_graph(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì±—ë´‡ ì‹¤í–‰\n",
    "\n",
    "ì´ì œ ì±—ë´‡ê³¼ ëŒ€í™”í•´ë´…ì‹œë‹¤!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.messages import stream_graph\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# ì§ˆë¬¸ ì…ë ¥\n",
    "user_input = \"ì•ˆë…•í•˜ì„¸ìš”! LangGraphì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”.\"\n",
    "\n",
    "# Config ì„¤ì •(recursion_limit: ì¬ê·€ ê¹Šì´ ì œí•œ, thread_id: ìŠ¤ë ˆë“œ ì•„ì´ë””)\n",
    "config = RunnableConfig(recursion_limit=20, thread_id=\"abc123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    \"messages\": [HumanMessage(content=\"ì•ˆë…•í•˜ì„¸ìš”! LangGraphì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”.\")]\n",
    "}\n",
    "\n",
    "# ê·¸ë˜í”„ ìŠ¤íŠ¸ë¦¬ë°\n",
    "stream_graph(graph, inputs=inputs, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: ë„êµ¬(Tools) ì¶”ê°€ ğŸ”§\n",
    "\n",
    "ì±—ë´‡ì´ \"ê¸°ì–µ\"ì—ì„œ ë‹µí•  ìˆ˜ ì—†ëŠ” ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ì›¹ ê²€ìƒ‰ ë„êµ¬ë¥¼ í†µí•©í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì±—ë´‡ì´ ì‹¤ì‹œê°„ ì •ë³´ë¥¼ ì°¾ì•„ ë” ë‚˜ì€ ì‘ë‹µì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "## í•µì‹¬ ê°œë…\n",
    "\n",
    "- **Tool Binding**: LLMì— ë„êµ¬ ì‚¬ìš© ë°©ë²• ì•Œë ¤ì£¼ê¸°\n",
    "- **Tool Node**: ë„êµ¬ë¥¼ ì‹¤í–‰í•˜ëŠ” ë…¸ë“œ\n",
    "- **Conditional Edges**: ì¡°ê±´ì— ë”°ë¼ ë‹¤ë¥¸ ë…¸ë“œë¡œ ë¼ìš°íŒ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "# Tavily ê²€ìƒ‰ ë„êµ¬ ì„¤ì •\n",
    "tool = TavilySearch(max_results=2)\n",
    "tools = [tool]\n",
    "\n",
    "# ë„êµ¬ í…ŒìŠ¤íŠ¸\n",
    "result = tool.invoke(\"LangGraphë€ ë¬´ì—‡ì¸ê°€ìš”?\")\n",
    "print(f\"ê²€ìƒ‰ ê²°ê³¼ ìˆ˜: {len(result['results'])}ê°œ\")\n",
    "print(f\"ì²« ë²ˆì§¸ ê²°ê³¼ ì œëª©: {result['results'][0]['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ê·¸ë˜í”„ êµ¬ì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "# State ì •ì˜ (ë™ì¼)\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "# ìƒˆë¡œìš´ ê·¸ë˜í”„ ë¹Œë” ìƒì„±\n",
    "builder = StateGraph(State)\n",
    "\n",
    "# LLMì— ë„êµ¬ ë°”ì¸ë”© - LLMì´ ë„êµ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì„¤ì •\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "\n",
    "def chatbot(state: State):\n",
    "    \"\"\"ë„êµ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì±—ë´‡ ë…¸ë“œ\"\"\"\n",
    "    # ë„êµ¬ê°€ ë°”ì¸ë”©ëœ LLM í˜¸ì¶œ\n",
    "    response = llm_with_tools.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# ë…¸ë“œ ì¶”ê°€\n",
    "builder.add_node(\"chatbot\", chatbot)\n",
    "\n",
    "# ToolNode ì¶”ê°€ - ë„êµ¬ë¥¼ ì‹¤í–‰í•˜ëŠ” ë…¸ë“œ\n",
    "tool_node = ToolNode(tools=tools)\n",
    "builder.add_node(\"tools\", tool_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì¡°ê±´ë¶€ ì—£ì§€(Conditional Edges) ì¶”ê°€\n",
    "\n",
    "ì±—ë´‡ì´ ë„êµ¬ë¥¼ í˜¸ì¶œí•´ì•¼ í• ì§€ ì§ì ‘ ì‘ë‹µí• ì§€ ê²°ì •í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ” tools_condition í•¨ìˆ˜ ìƒì„¸ ì„¤ëª…\n",
    "\n",
    "`tools_condition`ì€ LangGraphì˜ ì‚¬ì „ ì •ì˜ëœ ì¡°ê±´ í•¨ìˆ˜ë¡œ, LLMì˜ ì‘ë‹µì— ë„êµ¬ í˜¸ì¶œì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "\n",
    "#### ë™ì‘ ë°©ì‹\n",
    "\n",
    "```python\n",
    "def tools_condition(state) -> Literal[\"tools\", \"__end__\"]: \n",
    "    \"\"\"ë§ˆì§€ë§‰ ë©”ì‹œì§€ì— tool_callsê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ì—¬ ë¼ìš°íŒ…ì„ ê²°ì •í•©ë‹ˆë‹¤.\"\"\"\n",
    "    \n",
    "    # 1. ìƒíƒœì—ì„œ ë§ˆì§€ë§‰ ë©”ì‹œì§€ ì¶”ì¶œ\n",
    "    if isinstance(state, list):\n",
    "        ai_message = state[-1]\n",
    "    elif isinstance(state, dict) and \"messages\" in state:\n",
    "        ai_message = state[\"messages\"][-1]\n",
    "    \n",
    "    # 2. ë„êµ¬ í˜¸ì¶œ ì—¬ë¶€ í™•ì¸\n",
    "    if hasattr(ai_message, \"tool_calls\") and len(ai_message.tool_calls) > 0:\n",
    "        return \"tools\"  # ë„êµ¬ ë…¸ë“œë¡œ ë¼ìš°íŒ…\n",
    "    \n",
    "    return \"__end__\"  # ì¢…ë£Œ\n",
    "```\n",
    "\n",
    "#### í•µì‹¬ í¬ì¸íŠ¸\n",
    "\n",
    "1. **ìë™ íŒë‹¨**: LLMì´ ë„êµ¬ë¥¼ í˜¸ì¶œí•´ì•¼ í•œë‹¤ê³  íŒë‹¨í•˜ë©´ `tool_calls` í•„ë“œê°€ ìƒì„±ë©ë‹ˆë‹¤\n",
    "2. **ì¡°ê±´ë¶€ ë¼ìš°íŒ…**: \n",
    "   - ë„êµ¬ í˜¸ì¶œì´ ìˆìœ¼ë©´ â†’ `\"tools\"` ë…¸ë“œë¡œ ì´ë™\n",
    "   - ë„êµ¬ í˜¸ì¶œì´ ì—†ìœ¼ë©´ â†’ `\"__end__\"`ë¡œ ì´ë™ (ê·¸ë˜í”„ ì¢…ë£Œ)\n",
    "3. **ìœ ì—°í•œ ìƒíƒœ ì²˜ë¦¬**: ë¦¬ìŠ¤íŠ¸, ë”•ì…”ë„ˆë¦¬, BaseModel ë“± ë‹¤ì–‘í•œ ìƒíƒœ í˜•ì‹ ì§€ì›\n",
    "\n",
    "#### ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ\n",
    "\n",
    "```python\n",
    "# LLMì´ ê²€ìƒ‰ì´ í•„ìš”í•˜ë‹¤ê³  íŒë‹¨í•œ ê²½ìš°\n",
    "# ai_message.tool_calls = [{\"name\": \"tavily_search\", \"args\": {\"query\": \"...\"}}, ...]\n",
    "# â†’ tools_conditionì€ \"tools\"ë¥¼ ë°˜í™˜\n",
    "\n",
    "# LLMì´ ì§ì ‘ ë‹µë³€í•  ìˆ˜ ìˆë‹¤ê³  íŒë‹¨í•œ ê²½ìš°\n",
    "# ai_message.tool_calls = []\n",
    "# â†’ tools_conditionì€ \"__end__\"ë¥¼ ë°˜í™˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¡°ê±´ë¶€ ì—£ì§€ ì¶”ê°€\n",
    "# tools_conditionì€ ë©”ì‹œì§€ì— tool_callsê°€ ìˆìœ¼ë©´ \"tools\"ë¡œ,\n",
    "# ì—†ìœ¼ë©´ ENDë¡œ ë¼ìš°íŒ…í•©ë‹ˆë‹¤\n",
    "builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    tools_condition,  # ì‚¬ì „ ì •ì˜ëœ ì¡°ê±´ í•¨ìˆ˜ ì‚¬ìš©\n",
    ")\n",
    "\n",
    "# ë„êµ¬ ì‹¤í–‰ í›„ ë‹¤ì‹œ ì±—ë´‡ìœ¼ë¡œ ëŒì•„ê°€ê¸°\n",
    "builder.add_edge(\"tools\", \"chatbot\")\n",
    "\n",
    "# ì‹œì‘ì  ì„¤ì •\n",
    "builder.add_edge(START, \"chatbot\")\n",
    "\n",
    "# ê·¸ë˜í”„ ì»´íŒŒì¼\n",
    "graph_with_tools = builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê·¸ë˜í”„ ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê·¸ë˜í”„ ì‹œê°í™”\n",
    "visualize_graph(graph_with_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ì±—ë´‡ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.messages import stream_graph\n",
    "\n",
    "stream_graph(\n",
    "    graph_with_tools,\n",
    "    inputs={\n",
    "        \"messages\": [HumanMessage(content=\"2025ë…„ LangGraph ì‚¬ìš© ì‚¬ë¡€ ì•Œë ¤ì£¼ì„¸ìš”.\")]\n",
    "    },\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: ë©”ëª¨ë¦¬ ì¶”ê°€ ğŸ’¾\n",
    "\n",
    "ì±—ë´‡ì´ ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ê¸°ì–µí•  ìˆ˜ ìˆë„ë¡ **ì²´í¬í¬ì¸íŒ…(Checkpointing)** ì„ ì¶”ê°€í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë©€í‹°í„´ ëŒ€í™”ê°€ ê°€ëŠ¥í•´ì§‘ë‹ˆë‹¤.\n",
    "\n",
    "## í•µì‹¬ ê°œë…\n",
    "\n",
    "- **Checkpointer**: ìƒíƒœë¥¼ ì €ì¥í•˜ê³  ë³µì›í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜\n",
    "- **Thread ID**: ëŒ€í™” ì„¸ì…˜ì„ êµ¬ë¶„í•˜ëŠ” ì‹ë³„ì\n",
    "- **Persistent State**: ì—¬ëŸ¬ í˜¸ì¶œì— ê±¸ì³ ìœ ì§€ë˜ëŠ” ìƒíƒœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.graph import StateGraph, MessagesState, START\n",
    "from langgraph.store.base import BaseStore\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_teddynote.memory import create_memory_extractor\n",
    "import uuid\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4.1\", temperature=0)\n",
    "memory_extractor = create_memory_extractor(model=\"gpt-4.1\")\n",
    "\n",
    "\n",
    "def call_model(\n",
    "    state: MessagesState,\n",
    "    config: RunnableConfig,\n",
    "    *,\n",
    "    store: BaseStore,\n",
    ") -> dict[str, Any]:\n",
    "    \"\"\"Call the LLM model and manage user memory.\n",
    "\n",
    "    Args:\n",
    "        state (MessagesState): The current state containing messages.\n",
    "        config (RunnableConfig): The runnable configuration.\n",
    "        store (BaseStore): The memory store.\n",
    "    \"\"\"\n",
    "    # ë§ˆì§€ë§‰ ë©”ì‹œì§€ì—ì„œ user_id ì¶”ì¶œ\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    namespace = (\"memories\", user_id)\n",
    "\n",
    "    print(namespace)\n",
    "\n",
    "    # ìœ ì €ì˜ ë©”ëª¨ë¦¬ ê²€ìƒ‰\n",
    "    memories = store.search(namespace, query=str(state[\"messages\"][-1].content))\n",
    "    info = \"\\n\".join([f\"{memory.key}: {memory.value}\" for memory in memories])\n",
    "    system_msg = f\"You are a helpful assistant talking to the user. User info: {info}\"\n",
    "\n",
    "    # ì‚¬ìš©ìê°€ ê¸°ì–µ ìš”ì²­ ì‹œ ë©”ëª¨ë¦¬ ì €ì¥\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if \"remember\" in last_message.content.lower():\n",
    "        result = memory_extractor.invoke({\"input\": str(state[\"messages\"][-1].content)})\n",
    "        for memory in result.memories:\n",
    "            print(memory)\n",
    "            print(\"-\" * 100)\n",
    "            store.put(namespace, str(uuid.uuid4()), {memory.key: memory.value})\n",
    "\n",
    "    # LLM í˜¸ì¶œ\n",
    "    response = model.invoke(\n",
    "        [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"]\n",
    "    )\n",
    "    return {\"messages\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "# ê·¸ë˜í”„ ë¹Œë“œ\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"call_model\", call_model)\n",
    "builder.add_edge(START, \"call_model\")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì²´í¬í¬ì¸í„° ìƒì„±\n",
    "# ì‹¤ì œ í”„ë¡œë•ì…˜ì—ì„œëŠ” PostgresSaver ì‚¬ìš© ê¶Œì¥\n",
    "memory_saver = InMemorySaver()\n",
    "memory_store = InMemoryStore()\n",
    "\n",
    "# ê·¸ë˜í”„ ì»´íŒŒì¼\n",
    "graph_with_memory = builder.compile(\n",
    "    checkpointer=memory_saver,\n",
    "    store=memory_store,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.messages import stream_graph\n",
    "\n",
    "\n",
    "def run_graph(\n",
    "    msg,\n",
    "    thread_id=\"default\",\n",
    "    user_id=\"default\",\n",
    "):\n",
    "    config = {\n",
    "        \"configurable\": {\n",
    "            \"thread_id\": thread_id + user_id,\n",
    "            \"user_id\": user_id,\n",
    "        }\n",
    "    }\n",
    "    print(f\"\\n[ìœ ì €ğŸ™‹] {msg}\")\n",
    "    stream_graph(\n",
    "        graph_with_memory,\n",
    "        inputs={\"messages\": [{\"role\": \"user\", \"content\": msg}]},\n",
    "        config=config,\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©”ì‹œì§€, thread_id, user_id ì „ë‹¬\n",
    "run_graph(\"ì•ˆë…•? ë‚´ ì´ë¦„ì€ í…Œë””ì•¼\", \"1\", \"someone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©”ì‹œì§€, thread_id, user_id ì „ë‹¬\n",
    "run_graph(\"ë‚´ ì´ë¦„ì´ ë­ë¼ê³ ?\", \"1\", \"someone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©”ì‹œì§€, thread_id, user_id ì „ë‹¬\n",
    "run_graph(\"ë‚´ ì´ë¦„ì´ ë­ë¼ê³ ?\", \"2\", \"someone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`remember` ë¼ëŠ” í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì—¬ Long-term memoryì— ì •ë³´ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©”ì‹œì§€, thread_id, user_id ì „ë‹¬\n",
    "run_graph(\"ë‚´ ì´ë¦„ì´ í…Œë””ì•¼ remember\", \"2\", \"someone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ë ‡ê²Œ ì €ì¥í•œ Long-term memoryë¥¼ í™œìš©í•  ë•ŒëŠ” thread_id ê°€ ë°”ë€Œì–´ë„ ì£¼ìš” ì •ë³´ë¥¼ ê¸°ì–µí•˜ê³  ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©”ì‹œì§€, thread_id, user_id ì „ë‹¬\n",
    "run_graph(\"ë‚´ ì´ë¦„ì´ ë­ë¼ê³  í–ˆë”ë¼?\", \"3\", \"someone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë©”ì‹œì§€, thread_id, user_id ì „ë‹¬\n",
    "run_graph(\n",
    "    \"ë‚´ ì§ì—…ì€ AI Engineer ì•¼. ë‚´ ì·¨ë¯¸ëŠ” Netflix ë³´ê¸° ì•¼. remember\", \"4\", \"someone\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¤ë¥¸ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰\n",
    "run_graph(\"ë‚´ ì´ë¦„, ì§ì—…, ì·¨ë¯¸ ì•Œë ¤ì¤˜\", \"100\", \"someone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¤ë¥¸ user_id ë¡œ ì‹¤í–‰í•œ ê²½ìš°\n",
    "run_graph(\"ë‚´ ì´ë¦„, ì§ì—…, ì·¨ë¯¸ ì•Œë ¤ì¤˜\", \"100\", \"other\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State í™•ì¸\n",
    "\n",
    "`get_state` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì €ì¥ëœ ìƒíƒœë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„ì˜ì˜ Config ì„¤ì •\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": \"100\" + \"someone\",\n",
    "        \"user_id\": \"someone\",\n",
    "    }\n",
    "}\n",
    "\n",
    "# í˜„ì¬ ìƒíƒœ ê°€ì ¸ì˜¤ê¸°\n",
    "snapshot = graph_with_memory.get_state(config)\n",
    "\n",
    "print(\"ğŸ“Š í˜„ì¬ ìƒíƒœ ì •ë³´:\")\n",
    "print(f\"- ë©”ì‹œì§€ ìˆ˜: {len(snapshot.values['messages'])}ê°œ\")\n",
    "print(f\"- ì²´í¬í¬ì¸íŠ¸ ID: {snapshot.config['configurable']['checkpoint_id']}\")\n",
    "\n",
    "# ìµœê·¼ ë©”ì‹œì§€ ëª‡ ê°œ í‘œì‹œ\n",
    "print(\"\\n[ìµœê·¼ ë©”ì‹œì§€]\")\n",
    "for msg in snapshot.values[\"messages\"]:\n",
    "    role = msg.type if hasattr(msg, \"type\") else \"unknown\"\n",
    "    content = msg.content if hasattr(msg, \"content\") else str(msg)\n",
    "    print(f\"  [{role}]: {content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Human-in-the-Loop ğŸ™‹\n",
    "\n",
    "ì—ì´ì „íŠ¸ê°€ ì‹ ë¢°í•  ìˆ˜ ì—†ê±°ë‚˜ ì¤‘ìš”í•œ ê²°ì •ì„ ë‚´ë¦´ ë•Œ ì¸ê°„ì˜ ì…ë ¥ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. LangGraphì˜ **interrupt** ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì—¬ ì‹¤í–‰ì„ ì¼ì‹œ ì¤‘ì§€í•˜ê³  ì¸ê°„ì˜ í”¼ë“œë°±ì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "## í•µì‹¬ ê°œë…\n",
    "\n",
    "- **interrupt**: ì‹¤í–‰ì„ ì¼ì‹œ ì¤‘ì§€í•˜ëŠ” í•¨ìˆ˜\n",
    "- **Command**: ì‹¤í–‰ì„ ì¬ê°œí•˜ê³  ë°ì´í„°ë¥¼ ì „ë‹¬í•˜ëŠ” ê°ì²´\n",
    "- **Human Approval**: ì¸ê°„ ê²€í† /ìŠ¹ì¸ í”„ë¡œì„¸ìŠ¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langgraph.types import Command, interrupt\n",
    "\n",
    "\n",
    "@tool\n",
    "def human_assistance(query: str) -> str:\n",
    "    \"\"\"Request assistance from an expert(human).\"\"\"\n",
    "    # interruptë¥¼ í˜¸ì¶œí•˜ì—¬ ì‹¤í–‰ ì¼ì‹œ ì¤‘ì§€\n",
    "    # ì‚¬ëŒì˜ ì‘ë‹µì„ ê¸°ë‹¤ë¦¼\n",
    "    human_response = interrupt({\"query\": query})\n",
    "\n",
    "    # ì‚¬ëŒì˜ ì‘ë‹µ ë°˜í™˜\n",
    "    return human_response[\"data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human-in-the-Loopê°€ ìˆëŠ” ê·¸ë˜í”„ êµ¬ì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë„êµ¬ ë¦¬ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸\n",
    "tools_with_human = [human_assistance]\n",
    "\n",
    "# ìƒˆë¡œìš´ ê·¸ë˜í”„ êµ¬ì„±\n",
    "graph_builder_hitl = StateGraph(State)\n",
    "\n",
    "# LLMì— ë„êµ¬ ë°”ì¸ë”©\n",
    "llm_with_human_tools = llm.bind_tools(tools_with_human)\n",
    "\n",
    "\n",
    "def chatbot_with_human(state: State):\n",
    "    \"\"\"Human Interuption ìš”ì²­í•  ìˆ˜ ìˆëŠ” ì±—ë´‡\"\"\"\n",
    "    message = llm_with_human_tools.invoke(state[\"messages\"])\n",
    "\n",
    "    # interrupt ì¤‘ ë³‘ë ¬ ë„êµ¬ í˜¸ì¶œ ë°©ì§€\n",
    "    # (ì¬ê°œ ì‹œ ë„êµ¬ í˜¸ì¶œì´ ë°˜ë³µë˜ëŠ” ê²ƒì„ ë°©ì§€)\n",
    "    if hasattr(message, \"tool_calls\"):\n",
    "        assert (\n",
    "            len(message.tool_calls) <= 1\n",
    "        ), \"ë³‘ë ¬ ë„êµ¬ í˜¸ì¶œì€ interruptì™€ í•¨ê»˜ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤\"\n",
    "\n",
    "    return {\"messages\": [message]}\n",
    "\n",
    "\n",
    "# ë…¸ë“œ ì¶”ê°€\n",
    "graph_builder_hitl.add_node(\"chatbot_with_human\", chatbot_with_human)\n",
    "\n",
    "# ToolNode ì¶”ê°€\n",
    "tool_node_hitl = ToolNode(tools=tools_with_human)\n",
    "graph_builder_hitl.add_node(\"tools\", tool_node_hitl)\n",
    "\n",
    "# ì—£ì§€ ì¶”ê°€\n",
    "graph_builder_hitl.add_conditional_edges(\"chatbot_with_human\", tools_condition)\n",
    "graph_builder_hitl.add_edge(\"tools\", \"chatbot_with_human\")\n",
    "graph_builder_hitl.add_edge(START, \"chatbot_with_human\")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ì™€ í•¨ê»˜ ì»´íŒŒì¼\n",
    "memory_hitl = InMemorySaver()\n",
    "graph_hitl = graph_builder_hitl.compile(checkpointer=memory_hitl)\n",
    "\n",
    "# ê·¸ë˜í”„ ì‹œê°í™”\n",
    "visualize_graph(graph_hitl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human-in-the-Loop í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.messages import random_uuid\n",
    "\n",
    "# ì¸ê°„ ì§€ì›ì„ ìš”ì²­í•˜ëŠ” ë©”ì‹œì§€\n",
    "user_input = \"LangGraph ì˜í•˜ê³  ì‹¶ì€ë°, ì‚¬ëŒì—ê²Œ ì¡°ì–¸ì„ ë“£ê³  ì‹¶ì–´ìš”.\"\n",
    "config_hitl = {\"configurable\": {\"thread_id\": random_uuid()}}\n",
    "\n",
    "print(f\"User: {user_input}\\n\")\n",
    "\n",
    "stream_graph(\n",
    "    graph_hitl,\n",
    "    inputs={\"messages\": [HumanMessage(content=user_input)]},\n",
    "    config=config_hitl,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒíƒœ í™•ì¸ - ì–´ëŠ ë…¸ë“œì—ì„œ ì¤‘ë‹¨ë˜ì—ˆëŠ”ì§€ í™•ì¸\n",
    "snapshot = graph_hitl.get_state(config_hitl)\n",
    "print(f\"\\nğŸ“Š í˜„ì¬ ìƒíƒœ:\")\n",
    "print(f\"  ë‹¤ìŒ ì‹¤í–‰í•  ë…¸ë“œ: {snapshot.next}\")\n",
    "print(f\"  ì²´í¬í¬ì¸íŠ¸ ID: {snapshot.config['configurable']['checkpoint_id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¸ê°„ì˜ ì‘ë‹µìœ¼ë¡œ ì‹¤í–‰ ì¬ê°œ\n",
    "human_response = \"\"\"## ì „ë¬¸ê°€ì˜ ì¡°ì–¸: \n",
    "- YouTube í…Œë””ë…¸íŠ¸: https://www.youtube.com/c/teddynote\n",
    "- ê³ ê¸‰ ê°œë°œì ê°•ì˜ [íŒ¨ìŠ¤íŠ¸ìº í¼ìŠ¤ RAG ë¹„ë²•ë…¸íŠ¸](https://fastcampus.co.kr/data_online_teddy)\n",
    "\"\"\"\n",
    "\n",
    "# Command ê°ì²´ë¡œ ì¬ê°œ\n",
    "human_command = Command(resume={\"data\": human_response})\n",
    "\n",
    "print(f\"\\nğŸ’¡ ì‚¬ëŒì˜ ì‘ë‹µ: {human_response}\\n\")\n",
    "\n",
    "# ì¬ê°œ\n",
    "stream_graph(graph_hitl, inputs=human_command, config=config_hitl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: ìƒíƒœ ì»¤ìŠ¤í„°ë§ˆì´ì§• ğŸ¨\n",
    "\n",
    "ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ ì™¸ì— ì¶”ê°€ í•„ë“œë¥¼ ìƒíƒœì— ì¶”ê°€í•˜ì—¬ ë³µì¡í•œ ë™ì‘ì„ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, íŠ¹ì • ì •ë³´ë¥¼ ì €ì¥í•˜ê³  ì¸ê°„ì˜ ê²€í† ë¥¼ ë°›ëŠ” ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬í˜„í•´ë´…ì‹œë‹¤.\n",
    "\n",
    "## í•µì‹¬ ê°œë…\n",
    "\n",
    "- **Custom State Fields**: ìƒíƒœì— ì»¤ìŠ¤í…€ í•„ë“œ ì¶”ê°€\n",
    "- **State Updates from Tools**: ë„êµ¬ ë‚´ë¶€ì—ì„œ ìƒíƒœ ì—…ë°ì´íŠ¸\n",
    "- **Manual State Updates**: ìˆ˜ë™ìœ¼ë¡œ ìƒíƒœ ë³€ê²½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.tools import InjectedToolCallId\n",
    "\n",
    "\n",
    "# í™•ì¥ëœ State ì •ì˜\n",
    "class CustomState(TypedDict):\n",
    "    \"\"\"ì»¤ìŠ¤í…€ í•„ë“œê°€ ì¶”ê°€ëœ ìƒíƒœ\"\"\"\n",
    "\n",
    "    messages: Annotated[list, add_messages]\n",
    "    human_feedback: str  # ì‚¬ëŒì˜ í”¼ë“œë°±"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ë„êµ¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def human_review(\n",
    "    human_feedback, tool_call_id: Annotated[str, InjectedToolCallId]\n",
    ") -> str:\n",
    "    \"\"\"Request human review for information.\"\"\"\n",
    "    # ì¸ê°„ì—ê²Œ ê²€í†  ìš”ì²­\n",
    "    human_response = interrupt(\n",
    "        {\"question\": \"ì´ ì •ë³´ê°€ ë§ë‚˜ìš”?\", \"human_feedback\": human_feedback}\n",
    "    )\n",
    "\n",
    "    feedback = human_response.get(\"human_feedback\", \"\")\n",
    "\n",
    "    if feedback.strip() == \"\":\n",
    "        # ì‚¬ìš©ìê°€ AI ì˜ ë‹µë³€ì— ë™ì˜í•˜ëŠ” ê²½ìš°\n",
    "        return Command(\n",
    "            update={\n",
    "                \"messages\": [ToolMessage(human_response, tool_call_id=tool_call_id)]\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        # ì‚¬ìš©ìê°€ AI ì˜ ë‹µë³€ì— ë™ì˜í•˜ì§€ ì•ŠëŠ” ê²½ìš°\n",
    "        corrected_information = f\"# ì‚¬ìš©ìì— ì˜í•´ ìˆ˜ì •ëœ í”¼ë“œë°±: {feedback}\"\n",
    "        return Command(\n",
    "            update={\n",
    "                \"messages\": [\n",
    "                    ToolMessage(corrected_information, tool_call_id=tool_call_id)\n",
    "                ]\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì»¤ìŠ¤í…€ ìƒíƒœë¥¼ ì‚¬ìš©í•˜ëŠ” ê·¸ë˜í”„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë„êµ¬ ë¦¬ìŠ¤íŠ¸\n",
    "tools_custom = [human_review]\n",
    "\n",
    "# ìƒˆë¡œìš´ ê·¸ë˜í”„ êµ¬ì„±\n",
    "custom_graph_builder = StateGraph(CustomState)  # CustomState ì‚¬ìš©\n",
    "\n",
    "# LLMì— ë„êµ¬ ë°”ì¸ë”©\n",
    "llm_with_custom_tools = llm.bind_tools(tools_custom)\n",
    "\n",
    "\n",
    "def chatbot_custom(state: CustomState):\n",
    "    \"\"\"ì»¤ìŠ¤í…€ ìƒíƒœë¥¼ ì‚¬ìš©í•˜ëŠ” ì±—ë´‡\"\"\"\n",
    "    message = llm_with_custom_tools.invoke(state[\"messages\"])\n",
    "\n",
    "    if hasattr(message, \"tool_calls\"):\n",
    "        assert len(message.tool_calls) <= 1\n",
    "\n",
    "    return {\"messages\": [message]}\n",
    "\n",
    "\n",
    "# ë…¸ë“œì™€ ì—£ì§€ ì¶”ê°€\n",
    "custom_graph_builder.add_node(\"chatbot\", chatbot_custom)\n",
    "tool_node_custom = ToolNode(tools=tools_custom)\n",
    "custom_graph_builder.add_node(\"tools\", tool_node_custom)\n",
    "\n",
    "custom_graph_builder.add_conditional_edges(\"chatbot\", tools_condition)\n",
    "custom_graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "custom_graph_builder.add_edge(START, \"chatbot\")\n",
    "\n",
    "# ì»´íŒŒì¼\n",
    "memory_custom = InMemorySaver()\n",
    "custom_graph = custom_graph_builder.compile(checkpointer=memory_custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê·¸ë˜í”„ë¥¼ ì‹œê°í™” í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê·¸ë˜í”„ ì‹œê°í™”\n",
    "visualize_graph(custom_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì»¤ìŠ¤í…€ ìƒíƒœ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangGraphì˜ ì¶œì‹œì¼ì„ ì¡°ì‚¬í•˜ê³  ê²€í†  ìš”ì²­\n",
    "user_input = (\n",
    "    \"2024ë…„ ë…¸ë²¨ ë¬¸í•™ìƒ ìˆ˜ìƒìê°€ ëˆ„êµ¬ì¸ì§€ ì¡°ì‚¬í•´ì£¼ì„¸ìš”. \"\n",
    "    \"ë‹µì„ ì°¾ìœ¼ë©´ `human_review` ë„êµ¬ë¥¼ ì‚¬ìš©í•´ì„œ ê²€í† ë¥¼ ìš”ì²­í•˜ì„¸ìš”.\"\n",
    ")\n",
    "\n",
    "custom_config = RunnableConfig(configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "print(f\"User: {user_input}\\n\")\n",
    "\n",
    "# ì‹¤í–‰ (interruptì—ì„œ ì¤‘ë‹¨ë  ê²ƒì„)\n",
    "stream_graph(\n",
    "    custom_graph,\n",
    "    inputs={\"messages\": [HumanMessage(content=user_input)]},\n",
    "    config=custom_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.messages import display_message_tree\n",
    "\n",
    "# ìµœì‹  ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸°\n",
    "last_message = custom_graph.get_state(custom_config).values[\"messages\"][-1]\n",
    "\n",
    "# ìµœì‹  ë©”ì‹œì§€ tree êµ¬ì¡°ë¡œ í‘œì‹œ\n",
    "display_message_tree(last_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI ê°€ ì‘ì„±í•œ ë‚´ìš©\n",
    "print(last_message.tool_calls[0][\"args\"][\"human_feedback\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¸ê°„ì˜ ê²€í†  ì‘ë‹µìœ¼ë¡œ ì¬ê°œ\n",
    "human_command = Command(\n",
    "    resume={\"human_feedback\": \"2024ë…„ ë…¸ë²¨ ë¬¸í•™ìƒ ìˆ˜ìƒìëŠ” ëŒ€í•œë¯¼êµ­ì˜ í•œê°• ì‘ê°€ì…ë‹ˆë‹¤.\"}\n",
    ")\n",
    "\n",
    "stream_graph(custom_graph, inputs=human_command, config=custom_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: íƒ€ì„ íŠ¸ë˜ë¸” â°\n",
    "\n",
    "LangGraphì˜ **íƒ€ì„ íŠ¸ë˜ë¸”** ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë©´ ì´ì „ ì²´í¬í¬ì¸íŠ¸ë¡œ ëŒì•„ê°€ì„œ ë‹¤ë¥¸ ê²½ë¡œë¥¼ íƒìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ë””ë²„ê¹…, ì‹¤í—˜, ëŒ€í™”í˜• ì• í”Œë¦¬ì¼€ì´ì…˜ì— ë§¤ìš° ìœ ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "## í•µì‹¬ ê°œë…\n",
    "\n",
    "- **State History**: ëª¨ë“  ì²´í¬í¬ì¸íŠ¸ì˜ ê¸°ë¡\n",
    "- **Checkpoint ID**: íŠ¹ì • ì‹œì ì˜ ì‹ë³„ì\n",
    "- **Rewind**: ì´ì „ ìƒíƒœë¡œ ë˜ëŒë¦¬ê¸°\n",
    "- **Resume**: íŠ¹ì • ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### íƒ€ì„ íŠ¸ë˜ë¸”ì„ ìœ„í•œ ê·¸ë˜í”„ ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# íƒ€ì„ íŠ¸ë˜ë¸” í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ê°„ë‹¨í•œ ê·¸ë˜í”„\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# ë„êµ¬ì™€ LLM ì„¤ì •\n",
    "tools = [TavilySearch(max_results=2)]\n",
    "llm_with_tools_tt = llm.bind_tools(tools)\n",
    "\n",
    "\n",
    "def chatbot_tt(state: State):\n",
    "    \"\"\"íƒ€ì„ íŠ¸ë˜ë¸” í…ŒìŠ¤íŠ¸ìš© ì±—ë´‡\"\"\"\n",
    "    return {\"messages\": [llm_with_tools_tt.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "# ê·¸ë˜í”„ êµ¬ì„±\n",
    "graph_builder.add_node(\"chatbot\", chatbot_tt)\n",
    "tool_node_tt = ToolNode(tools=tools)\n",
    "graph_builder.add_node(\"tools\", tool_node_tt)\n",
    "\n",
    "graph_builder.add_conditional_edges(\"chatbot\", tools_condition)\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "\n",
    "# ë©”ëª¨ë¦¬ì™€ í•¨ê»˜ ì»´íŒŒì¼\n",
    "memory_tt = InMemorySaver()\n",
    "time_travel_graph = graph_builder.compile(checkpointer=memory_tt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì‹œê°í™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œê°í™”\n",
    "visualize_graph(time_travel_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì—¬ëŸ¬ ë‹¨ê³„ì˜ ëŒ€í™” ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_travel_config = RunnableConfig(configurable={\"thread_id\": \"time-travel-1\"})\n",
    "\n",
    "# ì²« ë²ˆì§¸ ëŒ€í™”\n",
    "stream_graph(\n",
    "    time_travel_graph,\n",
    "    inputs={\"messages\": [HumanMessage(content=\"í…Œë””ë…¸íŠ¸ì— ëŒ€í•´ì„œ ì¡°ì‚¬ ì¢€ í•´ì£¼ì„¸ìš”.\")]},\n",
    "    config=time_travel_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‘ ë²ˆì§¸ ëŒ€í™”\n",
    "stream_graph(\n",
    "    time_travel_graph,\n",
    "    inputs={\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=\"í…Œë””ë…¸íŠ¸ ì˜¨ë¼ì¸ ê°•ì˜ ì£¼ì†Œë¥¼ ì¡°ì‚¬í•´ í•´ì£¼ì„¸ìš”.\")\n",
    "        ]\n",
    "    },\n",
    "    config=time_travel_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ìƒíƒœ íˆìŠ¤í† ë¦¬ íƒìƒ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ì²´ ìƒíƒœ íˆìŠ¤í† ë¦¬ í™•ì¸\n",
    "print(\"ğŸ“œ ìƒíƒœ íˆìŠ¤í† ë¦¬ (ìµœì‹ ìˆœ):\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# to_replay ë³€ìˆ˜ ì´ˆê¸°í™”\n",
    "to_replay = None\n",
    "\n",
    "for i, state in enumerate(time_travel_graph.get_state_history(time_travel_config)):\n",
    "    print(f\"\\n[ì²´í¬í¬ì¸íŠ¸ {i}]\")\n",
    "    print(f\"  ë‹¤ìŒ ë…¸ë“œ: {state.next}\")\n",
    "    print(f\"  ì²´í¬í¬ì¸íŠ¸ ID: {state.config['configurable']['checkpoint_id']}\")\n",
    "\n",
    "    if len(state.values[\"messages\"]) == 6 and to_replay is None:\n",
    "        print(\"  â­ ì´ ìƒíƒœë¡œ ë˜ëŒì•„ê°ˆ ì˜ˆì •\")\n",
    "        display_message_tree(state.values[\"messages\"][-1])\n",
    "        to_replay = state\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### íŠ¹ì • ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_message_tree(to_replay.values[\"messages\"][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`update_tool_call` ì„ í™œìš©í•˜ì—¬ ê²€ìƒ‰í•  ì¿¼ë¦¬ë¥¼ ìˆ˜ì •í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.tools import update_tool_call\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ:\n",
    "updated_message = update_tool_call(\n",
    "    to_replay.values[\"messages\"][-1],\n",
    "    tool_name=\"tavily_search\",\n",
    "    tool_args={\"query\": \"í…Œë””ë…¸íŠ¸ ê°•ì˜ site:fastcampus.co.kr\", \"search_depth\": \"basic\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë³€ê²½í•˜ê¸° ì „ì˜ message\n",
    "display_message_tree(to_replay.values[\"messages\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë³€ê²½í•œ ì´í›„ì˜ ë©”ì‹œì§€ íŠ¸ë¦¬\n",
    "display_message_tree(updated_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë³€ê²½ëœ ë©”ì‹œì§€ë¥¼ update_state ë¡œ ì—…ë°ì´íŠ¸\n",
    "updated_state = time_travel_graph.update_state(\n",
    "    values={\"messages\": [updated_message]}, config=to_replay.config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì—…ë°ì´íŠ¸ëœ ë©”ì‹œì§€ë¥¼ ìŠ¤íŠ¸ë¦¬ë° í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œ `inputs` ëŠ” `None` ìœ¼ë¡œ ì£¼ê³ , `config` ëŠ” `updated_state` ë¡œ ì£¼ì–´ Replay í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—…ë°ì´íŠ¸ëœ ë©”ì‹œì§€ë¥¼ ìŠ¤íŠ¸ë¦¬ë° í•©ë‹ˆë‹¤.\n",
    "stream_graph(time_travel_graph, inputs=None, config=updated_state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
