{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# LangGraph Subgraph Tutorial\n\n## Purpose\n\nLearn how to build modular and reusable workflow components using LangGraph subgraphs for AI application development.\n\n### What You'll Learn\n\n- **Subgraph Fundamentals**: Composition patterns for complex AI workflows\n- **Shared State Management**: Coordinating data flow between graph components  \n- **Independent State Systems**: Isolation and transformation patterns\n- **Production Examples**: Real-world AI agent orchestration\n\n### Key Principles\n\n- **Modularity**: Break complex AI workflows into manageable components\n- **Reusability**: Build once, use across multiple AI applications\n- **Practical Focus**: Examples derived from production AI systems"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Table of Contents\n\n1. [Environment Setup](#environment-setup)\n2. [Part 1: Subgraph Fundamentals](#part-1-subgraph-fundamentals)\n3. [Part 2: Shared State Management](#part-2-shared-state-management)\n4. [Part 3: Independent State Systems](#part-3-independent-state-systems)\n5. [Part 4: Production Implementation](#part-4-production-implementation)\n6. [Summary](#summary)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Environment Setup\n\nConfigure the minimal environment for hands-on practice. API keys are managed through environment variables, with optional execution tracking."
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API KEY를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY 정보로드\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "LangGraph-Tutorial\n"
     ]
    }
   ],
   "source": [
    "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"LangGraph-Tutorial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# Part 1: Subgraph Fundamentals\n\n## Understanding Subgraphs\n\nA subgraph is an independent graph component that can be embedded within a larger graph structure. This pattern enables modular architecture in AI applications.\n\n### Core Concepts\n- **Component Isolation**: Each subgraph handles a specific processing task\n- **Reusability**: Components can be shared across different AI workflows\n- **Testability**: Independent testing of individual processing units\n\n### When to Use Subgraphs\n- Complex multi-stage AI pipelines\n- Reusable processing components\n- Team-based development with clear boundaries\n- Production systems requiring modular architecture\n\n### Implementation Example: Text Processing Pipeline\nWe'll build a text preprocessing system commonly used in NLP applications."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from typing_extensions import TypedDict\nfrom typing import List\nfrom langgraph.graph import StateGraph, START, END\nimport re\n\n# Define state for text processing pipeline\nclass ProcessingState(TypedDict):\n    text: str\n    tokens: List[str]\n\ndef create_text_preprocessor():\n    \"\"\"Text preprocessing subgraph for NLP applications\"\"\"\n    \n    subgraph = StateGraph(ProcessingState)\n    \n    def tokenize(state: ProcessingState):\n        \"\"\"Basic tokenization\"\"\"\n        tokens = state[\"text\"].lower().split()\n        return {\"tokens\": tokens}\n    \n    def normalize(state: ProcessingState):\n        \"\"\"Remove special characters and normalize\"\"\"\n        clean_tokens = [re.sub(r'[^\\w\\s]', '', token) for token in state[\"tokens\"]]\n        clean_tokens = [token for token in clean_tokens if token.strip()]\n        return {\"tokens\": clean_tokens}\n    \n    subgraph.add_node(\"tokenize\", tokenize)\n    subgraph.add_node(\"normalize\", normalize)\n    \n    subgraph.add_edge(START, \"tokenize\")\n    subgraph.add_edge(\"tokenize\", \"normalize\")\n    subgraph.add_edge(\"normalize\", END)\n    \n    return subgraph.compile()\n\n# Main processing pipeline\nmain_pipeline = StateGraph(ProcessingState)\n\ndef load_text(state: ProcessingState):\n    \"\"\"Input text loading\"\"\"\n    return {\"text\": f\"Input: {state['text']}\"}\n\ndef finalize_output(state: ProcessingState):\n    \"\"\"Generate final processed output\"\"\"\n    processed = \" \".join(state[\"tokens\"])\n    return {\"text\": f\"Processed: {processed}\"}\n\n# Add nodes to main pipeline\nmain_pipeline.add_node(\"load\", load_text)\nmain_pipeline.add_node(\"preprocessor\", create_text_preprocessor())\nmain_pipeline.add_node(\"finalize\", finalize_output)\n\n# Define execution flow\nmain_pipeline.add_edge(START, \"load\")\nmain_pipeline.add_edge(\"load\", \"preprocessor\")\nmain_pipeline.add_edge(\"preprocessor\", \"finalize\")\nmain_pipeline.add_edge(\"finalize\", END)\n\n# Compile and execute\npipeline = main_pipeline.compile()\n\n# Test with sample data\nresult = pipeline.invoke({\n    \"text\": \"Hello, World! This is a sample text for processing.\",\n    \"tokens\": []\n})\n\nprint(f\"Result: {result['text']}\")\nprint(f\"Tokens: {result['tokens']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Graph visualization demonstrates the modular structure."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langchain_teddynote.graphs import visualize_graph\n\n# Visualize the text processing pipeline structure\nprint(\"Text Processing Pipeline with Subgraph:\")\nvisualize_graph(pipeline, xray=True)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# Part 2: Shared State Management\n\n## Shared State Pattern\n\nIn shared state management, parent and subgraphs access the same state variables. This pattern enables seamless data coordination across different processing stages.\n\n### Key Characteristics\n- **Unified State**: All components access the same state variables\n- **Real-time Updates**: Changes propagate immediately across components\n- **Simple Integration**: No transformation layers required\n\n### Use Cases\n- Multi-stage AI pipelines sharing intermediate results\n- Collaborative processing between different AI models\n- Progress tracking across complex workflows\n\n### Implementation Example: RAG (Retrieval-Augmented Generation) System\nWe'll build a RAG system where retrieval and generation components share state."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define shared state for RAG system\nclass RAGState(TypedDict):\n    query: str\n    documents: List[str]\n    answer: str\n\ndef create_retrieval_system():\n    \"\"\"Document retrieval subgraph\"\"\"\n    \n    builder = StateGraph(RAGState)\n    \n    def search_documents(state: RAGState):\n        \"\"\"Simulate document retrieval\"\"\"\n        query = state[\"query\"].lower()\n        \n        # Simple keyword-based retrieval simulation\n        mock_db = [\n            \"LangGraph is a framework for building stateful AI applications\",\n            \"Subgraphs enable modular architecture in complex workflows\",\n            \"RAG combines retrieval and generation for better AI responses\",\n            \"Python is widely used for AI application development\"\n        ]\n        \n        relevant_docs = [doc for doc in mock_db if any(word in doc.lower() for word in query.split())]\n        \n        return {\"documents\": relevant_docs if relevant_docs else mock_db[:2]}\n    \n    builder.add_node(\"search\", search_documents)\n    builder.add_edge(START, \"search\")\n    builder.add_edge(\"search\", END)\n    \n    return builder.compile()\n\ndef create_generation_system():\n    \"\"\"Answer generation subgraph\"\"\"\n    \n    builder = StateGraph(RAGState)\n    \n    def generate_answer(state: RAGState):\n        \"\"\"Generate answer based on retrieved documents\"\"\"\n        docs = state[\"documents\"]\n        query = state[\"query\"]\n        \n        # Simple answer generation (in production, use actual LLM)\n        context = \" \".join(docs)\n        answer = f\"Based on the retrieved information: {context[:100]}... Answer to '{query}'\"\n        \n        return {\"answer\": answer}\n    \n    builder.add_node(\"generate\", generate_answer)\n    builder.add_edge(START, \"generate\")\n    builder.add_edge(\"generate\", END)\n    \n    return builder.compile()\n\n# Main RAG pipeline\nrag_pipeline = StateGraph(RAGState)\n\ndef process_query(state: RAGState):\n    \"\"\"Initialize query processing\"\"\"\n    return {\"query\": f\"Query: {state['query']}\"}\n\ndef finalize_response(state: RAGState):\n    \"\"\"Finalize the response\"\"\"\n    return {\"answer\": f\"Final: {state['answer']}\"}\n\n# Add components to pipeline\nrag_pipeline.add_node(\"process\", process_query)\nrag_pipeline.add_node(\"retriever\", create_retrieval_system())\nrag_pipeline.add_node(\"generator\", create_generation_system())\nrag_pipeline.add_node(\"finalize\", finalize_response)\n\n# Define execution flow\nrag_pipeline.add_edge(START, \"process\")\nrag_pipeline.add_edge(\"process\", \"retriever\")\nrag_pipeline.add_edge(\"retriever\", \"generator\")\nrag_pipeline.add_edge(\"generator\", \"finalize\")\nrag_pipeline.add_edge(\"finalize\", END)\n\n# Compile and test\nrag_system = rag_pipeline.compile()\n\n# Execute with test query\nresult = rag_system.invoke({\n    \"query\": \"What is LangGraph?\",\n    \"documents\": [],\n    \"answer\": \"\"\n})\n\nprint(f\"Query: {result['query']}\")\nprint(f\"Retrieved docs: {len(result['documents'])} documents\")\nprint(f\"Answer: {result['answer']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize RAG system architecture\nprint(\"RAG System with Shared State:\")\nvisualize_graph(rag_system, xray=True)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# Part 3: Independent State Systems\n\n## Independent State Pattern\n\nIn independent state systems, parent and subgraphs use completely different state schemas. Data exchange occurs through explicit transformation layers.\n\n### Key Benefits\n- **Complete Encapsulation**: Internal implementation hidden from external components\n- **Independent Evolution**: Components can change without affecting others\n- **Domain Optimization**: Each component uses optimal data structures\n- **Security**: Sensitive internal state remains isolated\n\n### Transformation Flow\n1. Parent state transforms to subgraph input format\n2. Subgraph processes data independently\n3. Subgraph output transforms back to parent state\n\n### Implementation Example: LLM Processing Pipeline\nWe'll build a multi-stage LLM system where each stage has specialized state management."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Parent system state (user-facing)\nclass UserRequest(TypedDict):\n    input: str\n    output: str\n\n# LLM processing state (internal)\nclass LLMState(TypedDict):\n    prompt: str\n    response: str\n    metadata: dict\n\ndef create_llm_processor():\n    \"\"\"LLM processing subgraph with independent state\"\"\"\n    \n    builder = StateGraph(LLMState)\n    \n    def analyze_request(state: LLMState):\n        \"\"\"Analyze input and prepare processing metadata\"\"\"\n        metadata = {\n            \"length\": len(state[\"prompt\"]),\n            \"complexity\": \"high\" if len(state[\"prompt\"]) > 50 else \"low\"\n        }\n        return {\"metadata\": metadata}\n    \n    def process_with_llm(state: LLMState):\n        \"\"\"Simulate LLM processing\"\"\"\n        prompt = state[\"prompt\"]\n        complexity = state[\"metadata\"][\"complexity\"]\n        \n        # Simulate different processing based on complexity\n        if complexity == \"high\":\n            response = f\"Complex analysis: {prompt[:30]}... [detailed processing]\"\n        else:\n            response = f\"Simple response: {prompt}\"\n            \n        return {\"response\": response}\n    \n    def format_output(state: LLMState):\n        \"\"\"Format the final output\"\"\"\n        formatted = f\"Processed: {state['response']} (Length: {state['metadata']['length']})\"\n        return {\"response\": formatted}\n    \n    builder.add_node(\"analyze\", analyze_request)\n    builder.add_node(\"process\", process_with_llm)\n    builder.add_node(\"format\", format_output)\n    \n    builder.add_edge(START, \"analyze\")\n    builder.add_edge(\"analyze\", \"process\")\n    builder.add_edge(\"process\", \"format\")\n    builder.add_edge(\"format\", END)\n    \n    return builder.compile()\n\n# Main orchestration system\nmain_system = StateGraph(UserRequest)\n\ndef receive_input(state: UserRequest):\n    \"\"\"Process incoming user request\"\"\"\n    return {\"input\": f\"Received: {state['input']}\"}\n\ndef process_with_llm_system(state: UserRequest):\n    \"\"\"Process using LLM subgraph with state transformation\"\"\"\n    \n    # Transform user state to LLM state\n    llm_input = {\n        \"prompt\": state[\"input\"],\n        \"response\": \"\",\n        \"metadata\": {}\n    }\n    \n    # Execute LLM subgraph\n    llm_system = create_llm_processor()\n    llm_result = llm_system.invoke(llm_input)\n    \n    # Transform LLM result back to user state\n    return {\"output\": llm_result[\"response\"]}\n\ndef finalize_response(state: UserRequest):\n    \"\"\"Finalize user response\"\"\"\n    return {\"output\": f\"Complete: {state['output']}\"}\n\n# Build main system\nmain_system.add_node(\"receive\", receive_input)\nmain_system.add_node(\"llm_process\", process_with_llm_system)\nmain_system.add_node(\"finalize\", finalize_response)\n\nmain_system.add_edge(START, \"receive\")\nmain_system.add_edge(\"receive\", \"llm_process\")\nmain_system.add_edge(\"llm_process\", \"finalize\")\nmain_system.add_edge(\"finalize\", END)\n\n# Compile and test\nllm_pipeline = main_system.compile()\n\n# Test with different complexity inputs\ntest_cases = [\n    \"Simple query\",\n    \"This is a more complex query that requires detailed analysis and processing\"\n]\n\nfor test_input in test_cases:\n    result = llm_pipeline.invoke({\"input\": test_input, \"output\": \"\"})\n    print(f\"Input: {test_input}\")\n    print(f\"Output: {result['output']}\")\n    print(\"-\" * 50)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize LLM pipeline with independent states\nprint(\"LLM Processing Pipeline with Independent States:\")\nvisualize_graph(llm_pipeline, xray=True)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# Part 4: Production Implementation\n\n## AI Agent Orchestration\n\nBuilding production-ready AI systems requires orchestrating multiple specialized agents. Each agent handles specific domain expertise while coordinating through a central orchestrator.\n\n### System Architecture\n- **Research Agent**: Information gathering and analysis\n- **Processing Agent**: Data transformation and computation\n- **Reporting Agent**: Result compilation and formatting\n- **Orchestrator**: Task routing and workflow coordination\n\n### Production Benefits\n- **Scalability**: Independent scaling of specialized components\n- **Maintainability**: Clear separation of concerns\n- **Extensibility**: Easy addition of new agent types\n- **Reliability**: Isolated failure domains\n\n### Implementation Example: AI Research Pipeline\nWe'll build a multi-agent system for automated research and analysis tasks."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Shared state for agent coordination\nclass AgentState(TypedDict):\n    task: str\n    result: str\n    agent_type: str\n\ndef create_research_agent():\n    \"\"\"Research agent subgraph\"\"\"\n    \n    builder = StateGraph(AgentState)\n    \n    def gather_information(state: AgentState):\n        \"\"\"Simulate information gathering\"\"\"\n        task = state[\"task\"]\n        \n        # Simulate research based on task content\n        if \"analysis\" in task.lower():\n            result = f\"Research findings: {task} - Found 3 relevant studies, 5 key metrics\"\n        elif \"comparison\" in task.lower():\n            result = f\"Comparative analysis: {task} - Identified 4 alternatives, pros/cons matrix\"\n        else:\n            result = f\"General research: {task} - Collected background information, key concepts\"\n        \n        return {\"result\": result, \"agent_type\": \"research\"}\n    \n    builder.add_node(\"research\", gather_information)\n    builder.add_edge(START, \"research\")\n    builder.add_edge(\"research\", END)\n    \n    return builder.compile()\n\ndef create_processing_agent():\n    \"\"\"Data processing agent subgraph\"\"\"\n    \n    builder = StateGraph(AgentState)\n    \n    def process_data(state: AgentState):\n        \"\"\"Process and analyze data\"\"\"\n        task = state[\"task\"]\n        \n        # Simulate data processing\n        result = f\"Processing complete: {task} - Data normalized, patterns identified, metrics calculated\"\n        \n        return {\"result\": result, \"agent_type\": \"processing\"}\n    \n    builder.add_node(\"process\", process_data)\n    builder.add_edge(START, \"process\")\n    builder.add_edge(\"process\", END)\n    \n    return builder.compile()\n\ndef create_reporting_agent():\n    \"\"\"Report generation agent subgraph\"\"\"\n    \n    builder = StateGraph(AgentState)\n    \n    def generate_report(state: AgentState):\n        \"\"\"Generate final report\"\"\"\n        previous_result = state.get(\"result\", \"\")\n        \n        report = f\"Executive Summary: {previous_result[:50]}... [Full report with charts and recommendations]\"\n        \n        return {\"result\": report, \"agent_type\": \"reporting\"}\n    \n    builder.add_node(\"report\", generate_report)\n    builder.add_edge(START, \"report\")\n    builder.add_edge(\"report\", END)\n    \n    return builder.compile()\n\n# Main orchestration system\norchestrator = StateGraph(AgentState)\n\ndef classify_task(state: AgentState):\n    \"\"\"Classify incoming task\"\"\"\n    task = state[\"task\"].lower()\n    \n    if \"research\" in task or \"analyze\" in task:\n        agent_type = \"research\"\n    elif \"process\" in task or \"compute\" in task:\n        agent_type = \"processing\"\n    else:\n        agent_type = \"research\"  # Default to research\n    \n    return {\"agent_type\": agent_type}\n\ndef route_to_agent(state: AgentState):\n    \"\"\"Route task to appropriate agent\"\"\"\n    agent_type = state[\"agent_type\"]\n    \n    if agent_type == \"research\":\n        research_agent = create_research_agent()\n        result = research_agent.invoke(state)\n        return {\"result\": result[\"result\"], \"agent_type\": result[\"agent_type\"]}\n    else:\n        processing_agent = create_processing_agent()\n        result = processing_agent.invoke(state)\n        return {\"result\": result[\"result\"], \"agent_type\": result[\"agent_type\"]}\n\ndef finalize_with_report(state: AgentState):\n    \"\"\"Generate final report\"\"\"\n    reporting_agent = create_reporting_agent()\n    result = reporting_agent.invoke(state)\n    return {\"result\": result[\"result\"]}\n\n# Build orchestration pipeline\norchestrator.add_node(\"classify\", classify_task)\norchestrator.add_node(\"route\", route_to_agent)\norchestrator.add_node(\"report\", finalize_with_report)\n\norchestrator.add_edge(START, \"classify\")\norchestrator.add_edge(\"classify\", \"route\")\norchestrator.add_edge(\"route\", \"report\")\norchestrator.add_edge(\"report\", END)\n\n# Compile and test\nagent_system = orchestrator.compile()\n\n# Test with different task types\ntest_tasks = [\n    \"Analyze market trends for AI startups\",\n    \"Process customer feedback data\",\n    \"Research competitive landscape\"\n]\n\nprint(\"AI Agent Orchestration Results:\")\nprint(\"=\" * 50)\n\nfor task in test_tasks:\n    result = agent_system.invoke({\n        \"task\": task,\n        \"result\": \"\",\n        \"agent_type\": \"\"\n    })\n    \n    print(f\"Task: {task}\")\n    print(f\"Agent: {result['agent_type']}\")\n    print(f\"Result: {result['result'][:80]}...\")\n    print(\"-\" * 50)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize agent orchestration system\nprint(\"AI Agent Orchestration System:\")\nvisualize_graph(agent_system, xray=True)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n# Summary\n\n## Concepts Covered\n\nYou have successfully learned the core concepts of LangGraph subgraphs for AI application development.\n\n### Key Learning Outcomes\n\n1. **Subgraph Fundamentals**\n   - Modular architecture for AI workflows\n   - Component isolation and reusability\n   - Production-ready design patterns\n\n2. **Shared State Management**\n   - Coordinated data flow between components\n   - Real-time state synchronization\n   - Efficient inter-component communication\n\n3. **Independent State Systems**\n   - Encapsulated component design\n   - Transformation-based data exchange\n   - Security through isolation\n\n4. **Production Implementation**\n   - Multi-agent orchestration patterns\n   - Scalable AI system architecture\n   - Enterprise-ready workflow design\n\n## Next Steps\n\n### For Intermediate Developers\n- Implement error handling and recovery mechanisms\n- Add monitoring and observability layers\n- Optimize for high-throughput scenarios\n\n### For Advanced Practitioners\n- Design distributed agent systems\n- Implement advanced routing strategies\n- Build production deployment pipelines\n\n### For System Architects\n- Plan enterprise AI architectures\n- Design multi-tenant agent systems\n- Implement governance and compliance frameworks\n\n## Best Practices\n\n1. **Design Principles**: Single responsibility, loose coupling, high cohesion\n2. **Testing Strategy**: Unit test individual subgraphs, integration test workflows\n3. **Monitoring**: Implement comprehensive logging and metrics collection\n4. **Documentation**: Maintain clear interface specifications and architectural diagrams\n\n## Production Readiness\n\nYou now have the foundation to build production-grade AI applications using LangGraph subgraphs. Apply these patterns to create scalable, maintainable, and robust AI systems.\n\nFor advanced features and production deployment guidance, refer to the official LangGraph documentation."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}