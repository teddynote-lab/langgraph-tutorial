{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph MCP (Model Context Protocol) íŠœí† ë¦¬ì–¼\n",
    "\n",
    "ì´ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” LangGraphì™€ MCP(Model Context Protocol)ë¥¼ í†µí•©í•˜ì—¬ ê°•ë ¥í•œ AI ì—ì´ì „íŠ¸ë¥¼ êµ¬ì¶•í•˜ëŠ” ë°©ë²•ì„ ë°°ì›ë‹ˆë‹¤.\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ\n",
    "- MCPì˜ ê°œë…ê³¼ ì•„í‚¤í…ì²˜ ì´í•´\n",
    "- MultiServerMCPClientë¥¼ ì‚¬ìš©í•œ ë‹¤ì¤‘ ì„œë²„ ê´€ë¦¬\n",
    "- React Agent ë° ToolNodeì™€ MCP í†µí•©\n",
    "- ì‹¤ì „ ì˜ˆì œë¥¼ í†µí•œ ë³µì¡í•œ ì—ì´ì „íŠ¸ êµ¬ì¶•\n",
    "\n",
    "## ëª©ì°¨\n",
    "1. **MCP ê°œìš” ë° ì„¤ì¹˜**\n",
    "2. **ê¸°ë³¸ MCP ì„œë²„ ìƒì„±**\n",
    "3. **MultiServerMCPClient ì„¤ì •**\n",
    "4. **React Agentì™€ MCP í†µí•©**\n",
    "5. **ToolNodeì™€ MCP í†µí•©**\n",
    "6. **ë‹¤ì¤‘ MCP ì„œë²„ ê´€ë¦¬**\n",
    "7. **ì‹¤ì „ ì˜ˆì œ - ë³µì¡í•œ ì—ì´ì „íŠ¸ êµ¬ì¶•**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: MCP ê¸°ë³¸ ê°œë…\n",
    "\n",
    "### MCP(Model Context Protocol)ë€?\n",
    "\n",
    "MCPëŠ” ì• í”Œë¦¬ì¼€ì´ì…˜ì´ ì–¸ì–´ ëª¨ë¸ì— ë„êµ¬ì™€ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì œê³µí•˜ëŠ” ë°©ë²•ì„ í‘œì¤€í™”í•œ ì˜¤í”ˆ í”„ë¡œí† ì½œì…ë‹ˆë‹¤.\n",
    "\n",
    "#### ì£¼ìš” íŠ¹ì§•:\n",
    "- ğŸ”§ **í‘œì¤€í™”ëœ ë„êµ¬ ì¸í„°í˜ì´ìŠ¤**: ì¼ê´€ëœ ë°©ì‹ìœ¼ë¡œ ë„êµ¬ ì •ì˜ ë° ì‚¬ìš©\n",
    "- ğŸŒ **ë‹¤ì–‘í•œ ì „ì†¡ ë©”ì»¤ë‹ˆì¦˜**: stdio, HTTP, WebSocket ë“± ì§€ì›\n",
    "- ğŸ”„ **ë™ì  ë„êµ¬ ê²€ìƒ‰**: ëŸ°íƒ€ì„ì— ë„êµ¬ ìë™ ê²€ìƒ‰ ë° ë¡œë“œ\n",
    "- ğŸ—ï¸ **í™•ì¥ ê°€ëŠ¥í•œ ì•„í‚¤í…ì²˜**: ì—¬ëŸ¬ ì„œë²„ë¥¼ ë™ì‹œì— ì—°ê²° ê°€ëŠ¥\n",
    "\n",
    "### ì„¤ì¹˜\n",
    "\n",
    "MCPë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "# MCP\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# ë¹„ë™ê¸° í˜¸ì¶œ í™œì„±í™”\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangSmith ì¶”ì ì„ ì„¤ì •í•©ë‹ˆë‹¤. https://smith.langchain.com\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ì´ë¦„ì„ ì…ë ¥í•©ë‹ˆë‹¤.\n",
    "logging.langsmith(\"LangGraph-MCP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: ê¸°ë³¸ MCP ì„œë²„ ìƒì„±\n",
    "\n",
    "MCP ì„œë²„ëŠ” ë„êµ¬ë¥¼ ì œê³µí•˜ëŠ” ë…ë¦½ì ì¸ í”„ë¡œì„¸ìŠ¤ì…ë‹ˆë‹¤. FastMCPë¥¼ ì‚¬ìš©í•˜ì—¬ ê°„ë‹¨í•œ ì„œë²„ë¥¼ ë§Œë“¤ì–´ë´…ì‹œë‹¤.\n",
    "\n",
    "- `server/mcp_server_local.py`\n",
    "- `server/mcp_server_rag.py`\n",
    "- `server/mcp_server_remote.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: MultiServerMCPClient ì„¤ì •\n",
    "\n",
    "MultiServerMCPClientë¥¼ ì‚¬ìš©í•˜ë©´ ì—¬ëŸ¬ MCP ì„œë²„ë¥¼ ë™ì‹œì— ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiServerMCPClient ì„¤ì • ì˜ˆì œ\n",
    "async def setup_mcp_client(server_configs: List[Dict[str, Any]]):\n",
    "    \"\"\"MCP í´ë¼ì´ì–¸íŠ¸ë¥¼ ì„¤ì •í•˜ê³  ë„êµ¬ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\"\"\"\n",
    "\n",
    "    # MCP í´ë¼ì´ì–¸íŠ¸ ìƒì„±\n",
    "    client = MultiServerMCPClient(server_configs)\n",
    "\n",
    "    # ë„êµ¬ ê°€ì ¸ì˜¤ê¸°\n",
    "    tools = await client.get_tools()\n",
    "\n",
    "    print(f\"âœ… {len(tools)} ê°œì˜ MCP ë„êµ¬ê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤:\")\n",
    "    for tool in tools:\n",
    "        print(f\"  - {tool.name}\")\n",
    "\n",
    "    return client, tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„œë²„ êµ¬ì„± ì •ì˜\n",
    "server_configs = {\n",
    "    \"weather\": {\n",
    "        \"command\": \"uv\",\n",
    "        \"args\": [\"run\", \"python\", \"server/mcp_server_local.py\"],\n",
    "        \"transport\": \"stdio\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# MCP í´ë¼ì´ì–¸íŠ¸ ìƒì„±\n",
    "client, tools = await setup_mcp_client(server_configs=server_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM ì„¤ì •\n",
    "llm = ChatOpenAI(model=\"gpt-4.1\", temperature=0)\n",
    "\n",
    "# React Agent ìƒì„±\n",
    "agent = create_react_agent(\n",
    "    llm, tools, checkpointer=InMemorySaver()  # ìƒíƒœ ì €ì¥ì„ ìœ„í•œ ì²´í¬í¬ì¸í„°\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.messages import astream_graph, random_uuid\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "config = RunnableConfig(configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "response = await astream_graph(\n",
    "    agent,\n",
    "    inputs={\"messages\": [(\"human\", \"ì•ˆë…•í•˜ì„¸ìš”. ì„œìš¸ì˜ ë‚ ì”¨ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.\")]},\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTTP ì „ì†¡ ë°©ì‹ ì‚¬ìš©\n",
    "\n",
    "ì›ê²© ì„œë²„ë‚˜ HTTP ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ë¨¼ì € Remote MCP ì„œë²„ë¥¼ êµ¬ë™í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "```bash\n",
    "uv run python server/mcp_server_remote.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTTP ê¸°ë°˜ MCP ì„œë²„ ì„¤ì • ì˜ˆì œ\n",
    "http_server_config = {\n",
    "    \"current_time\": {\n",
    "        \"url\": \"http://127.0.0.1:8002/mcp\",\n",
    "        \"transport\": \"streamable_http\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# MCP í´ë¼ì´ì–¸íŠ¸ ìƒì„±\n",
    "client, http_tools = await setup_mcp_client(server_configs=http_server_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM ì„¤ì •\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "\n",
    "# React Agent ìƒì„±\n",
    "agent = create_react_agent(\n",
    "    llm, http_tools, checkpointer=InMemorySaver()  # ìƒíƒœ ì €ì¥ì„ ìœ„í•œ ì²´í¬í¬ì¸í„°\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.messages import astream_graph, random_uuid\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "config = RunnableConfig(configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "response = await astream_graph(\n",
    "    agent,\n",
    "    inputs={\"messages\": [(\"human\", \"ì•ˆë…•í•˜ì„¸ìš”. í˜„ì¬ ì‹œê°„ì„ ì•Œë ¤ì£¼ì„¸ìš”.\")]},\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCP Inspector ì‚¬ìš©\n",
    "\n",
    "`npx @modelcontextprotocol/inspector` ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì—¬ ë¸Œë¼ìš°ì €ì—ì„œ MCP ì„œë²„ë¥¼ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![mcp_inspector](./assets/mcp-inspector.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTTP ê¸°ë°˜ MCP ì„œë²„ ì„¤ì • ì˜ˆì œ\n",
    "http_server_config = {\n",
    "    \"rag\": {\n",
    "        \"command\": \"uv\",\n",
    "        \"args\": [\"run\", \"python\", \"server/mcp_server_rag.py\"],\n",
    "        \"transport\": \"stdio\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# MCP í´ë¼ì´ì–¸íŠ¸ ìƒì„±\n",
    "client, rag_tools = await setup_mcp_client(server_configs=http_server_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM ì„¤ì •\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "\n",
    "# React Agent ìƒì„±\n",
    "rag_agent = create_react_agent(\n",
    "    llm, rag_tools, checkpointer=InMemorySaver()  # ìƒíƒœ ì €ì¥ì„ ìœ„í•œ ì²´í¬í¬ì¸í„°\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.messages import astream_graph, random_uuid\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "config = RunnableConfig(configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "\n",
    "_ = await astream_graph(\n",
    "    rag_agent,\n",
    "    inputs={\n",
    "        \"messages\": [\n",
    "            (\n",
    "                \"human\",\n",
    "                \"ì‚¼ì„±ì „ìê°€ ê°œë°œí•œ ìƒì„±í˜• AI ì˜ ì´ë¦„ì€? mcp ì„œë²„ë¥¼ ì‚¬ìš©í•´ì„œ ê²€ìƒ‰í•´ì£¼ì„¸ìš”.\",\n",
    "            )\n",
    "        ]\n",
    "    },\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = await astream_graph(\n",
    "    rag_agent,\n",
    "    inputs={\n",
    "        \"messages\": [\n",
    "            (\n",
    "                \"human\",\n",
    "                \"êµ¬ê¸€ì´ Anthropic ì— íˆ¬ìí•˜ê¸°ë¡œ í•œ ê¸ˆì•¡ì„ ê²€ìƒ‰í•´ì¤˜\",\n",
    "            )\n",
    "        ]\n",
    "    },\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: React Agentì™€ MCP í†µí•©\n",
    "\n",
    "React AgentëŠ” ì¶”ë¡ (Reason)ê³¼ í–‰ë™(Act)ì„ ë°˜ë³µí•˜ëŠ” íŒ¨í„´ì„ êµ¬í˜„í•©ë‹ˆë‹¤. MCP ë„êµ¬ì™€ í•¨ê»˜ ì‚¬ìš©í•˜ë©´ ê°•ë ¥í•œ ì—ì´ì „íŠ¸ë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def create_mcp_react_agent(server_configs: dict):\n",
    "    \"\"\"MCP ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ëŠ” React Agentë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\"\"\"\n",
    "\n",
    "    # MCP í´ë¼ì´ì–¸íŠ¸ ìƒì„± ë° ë„êµ¬ ê°€ì ¸ì˜¤ê¸°\n",
    "    client, tools = await setup_mcp_client(server_configs=server_configs)\n",
    "\n",
    "    # LLM ì„¤ì •\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "    # React Agent ìƒì„±\n",
    "    agent = create_react_agent(\n",
    "        llm, tools, checkpointer=InMemorySaver()  # ìƒíƒœ ì €ì¥ì„ ìœ„í•œ ì²´í¬í¬ì¸í„°\n",
    "    )\n",
    "\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCP ì„œë²„ Config ì„¤ì •\n",
    "server_configs = {\n",
    "    \"weather\": {\n",
    "        \"command\": \"uv\",\n",
    "        \"args\": [\"run\", \"python\", \"server/mcp_server_local.py\"],\n",
    "        \"transport\": \"stdio\",\n",
    "    },\n",
    "    \"current_time\": {\n",
    "        \"url\": \"http://127.0.0.1:8002/mcp\",\n",
    "        \"transport\": \"streamable_http\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# MCP ReAct Agent ìƒì„±\n",
    "agent = await create_mcp_react_agent(server_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_teddynote.messages import random_uuid, astream_graph\n",
    "\n",
    "\n",
    "config = RunnableConfig(configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "await astream_graph(\n",
    "    agent, inputs={\"messages\": [(\"human\", \"í˜„ì¬ ì‹œê°„ì„ ì•Œë ¤ì£¼ì„¸ìš”\")]}, config=config\n",
    ")\n",
    "\n",
    "await astream_graph(\n",
    "    agent,\n",
    "    inputs={\"messages\": [(\"human\", \"í˜„ì¬ ì„œìš¸ì˜ ë‚ ì”¨ë„ ì•Œë ¤ì£¼ì„¸ìš”\")]},\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: ToolNodeì™€ MCP í†µí•©\n",
    "\n",
    "ToolNodeë¥¼ ì‚¬ìš©í•˜ë©´ ë” ì„¸ë°€í•œ ì œì–´ê°€ ê°€ëŠ¥í•œ ì»¤ìŠ¤í…€ ì›Œí¬í”Œë¡œìš°ë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langchain_teddynote.graphs import visualize_graph\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage\n",
    "from typing import Annotated, List, Dict, Any\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import TypedDict\n",
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "\n",
    "# ìƒíƒœ ì •ì˜\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"ì—ì´ì „íŠ¸ ìƒíƒœ\"\"\"\n",
    "\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    context: Dict[str, Any] = None  # ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ ì •ë³´\n",
    "\n",
    "\n",
    "async def create_mcp_workflow(server_configs: dict):\n",
    "    \"\"\"MCP ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ì»¤ìŠ¤í…€ ì›Œí¬í”Œë¡œìš° ìƒì„±\"\"\"\n",
    "\n",
    "    # MCP í´ë¼ì´ì–¸íŠ¸ ìƒì„±\n",
    "    client, tools = await setup_mcp_client(server_configs=server_configs)\n",
    "\n",
    "    # Tavily ê²€ìƒ‰ ë„êµ¬ ì¶”ê°€\n",
    "    tool = TavilySearch(max_results=2)\n",
    "    tools.append(tool)\n",
    "\n",
    "    # LLM ì„¤ì • (ë„êµ¬ ë°”ì¸ë”©)\n",
    "    llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "    llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "    # ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ ìƒì„±\n",
    "    workflow = StateGraph(AgentState)\n",
    "\n",
    "    # ë…¸ë“œ ì •ì˜\n",
    "    async def agent_node(state: AgentState):\n",
    "        \"\"\"LLMì„ í˜¸ì¶œí•˜ì—¬ ì‘ë‹µ ìƒì„±\"\"\"\n",
    "        response = await llm_with_tools.ainvoke(state[\"messages\"])\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    # ToolNode ìƒì„±\n",
    "    tool_node = ToolNode(tools)\n",
    "\n",
    "    # ê·¸ë˜í”„ êµ¬ì„±\n",
    "    workflow.add_node(\"agent\", agent_node)\n",
    "    workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "    workflow.add_edge(START, \"agent\")\n",
    "    workflow.add_conditional_edges(\"agent\", tools_condition)\n",
    "    workflow.add_edge(\"tools\", \"agent\")\n",
    "    workflow.add_edge(\"agent\", END)\n",
    "\n",
    "    # ì»´íŒŒì¼\n",
    "    app = workflow.compile(checkpointer=InMemorySaver())\n",
    "\n",
    "    visualize_graph(app)\n",
    "\n",
    "    return app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCP ì„œë²„ Config ì„¤ì •\n",
    "server_configs = {\n",
    "    \"weather\": {\n",
    "        \"command\": \"uv\",\n",
    "        \"args\": [\"run\", \"python\", \"server/mcp_server_local.py\"],\n",
    "        \"transport\": \"stdio\",\n",
    "    },\n",
    "    \"current_time\": {\n",
    "        \"url\": \"http://127.0.0.1:8002/mcp\",\n",
    "        \"transport\": \"streamable_http\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcp_app = await create_mcp_workflow(server_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = await astream_graph(\n",
    "    mcp_app, inputs={\"messages\": [(\"human\", \"í˜„ì¬ ì‹œê°„ì„ ì•Œë ¤ì£¼ì„¸ìš”\")]}, config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = await astream_graph(\n",
    "    mcp_app,\n",
    "    inputs={\n",
    "        \"messages\": [\n",
    "            (\"human\", \"ì˜¤ëŠ˜ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•´ì£¼ì„¸ìš”. ê²€ìƒ‰ì‹œ ì‹œê°„ì„ ì¡°íšŒí•œ ë’¤ ì²˜ë¦¬í•˜ì„¸ìš”.\")\n",
    "        ]\n",
    "    },\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: ì™¸ë¶€ MCP ì„œë²„ì—ì„œ 3rd Party ë„êµ¬ ì‚¬ìš©í•˜ê¸°\n",
    "\n",
    "**Smithery AIë€?**\n",
    "\n",
    "- ì‚¬ì´íŠ¸: https://smithery.ai/\n",
    "\n",
    "- Smithery AIëŠ” AI ì—ì´ì „íŠ¸ ì„œë¹„ìŠ¤ì˜ í—ˆë¸Œ ì—­í• ì„ í•˜ëŠ” í”Œë«í¼ì…ë‹ˆë‹¤. \n",
    "\n",
    "ì—ì´ì „íŠ¸í˜• AI(ì˜ˆ: ëŒ€í˜• ì–¸ì–´ ëª¨ë¸)ê°€ ì™¸ë¶€ ë„êµ¬ë‚˜ ì •ë³´ì™€ íš¨ìœ¨ì ìœ¼ë¡œ ì—°ê²°ë  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ëœ MCP ì„œë²„ë“¤ì„ ê²€ìƒ‰í•˜ê³  ë°°í¬í•˜ëŠ” ì—­í• ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì¦‰, AIê°€ ë‹¤ì–‘í•œ ì™¸ë¶€ ì„œë¹„ìŠ¤ì™€ ì†ì‰½ê²Œ í†µì‹ í•  ìˆ˜ ìˆê²Œ í•˜ëŠ” ì¤‘ê°œìì´ì ìƒíƒœê³„ í—ˆë¸Œì…ë‹ˆë‹¤.\n",
    "\n",
    "ì´ ëª¨ë“  ì„œë¹„ìŠ¤ì˜ ì—°ê²°ì€ **MCP(Model Context Protocol)** í‘œì¤€ í”„ë¡œí† ì½œì„ ë”°ë¦…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ AIê°€ ê²€ìƒ‰, í”„ë¡œê·¸ë˜ë°, íŒŒì¼ ê´€ë¦¬, ë‹¤ì–‘í•œ API ì—°ë™ ë“± ì—¬ëŸ¬ ì™¸ë¶€ ê¸°ëŠ¥ì„ í•˜ë‚˜ì˜ í—ˆë¸Œ(í”Œë«í¼)ë§Œìœ¼ë¡œ í¸ë¦¬í•˜ê²Œ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCP ì„œë²„ Config ì„¤ì •\n",
    "server_configs = {\n",
    "    \"weather\": {\n",
    "        \"command\": \"uv\",\n",
    "        \"args\": [\"run\", \"python\", \"server/mcp_server_local.py\"],\n",
    "        \"transport\": \"stdio\",\n",
    "    },\n",
    "    \"current_time\": {\n",
    "        \"url\": \"http://127.0.0.1:8002/mcp\",\n",
    "        \"transport\": \"streamable_http\",\n",
    "    },\n",
    "    \"context7-mcp\": {\n",
    "        \"command\": \"npx\",\n",
    "        \"args\": [\n",
    "            \"-y\",\n",
    "            \"@smithery/cli@latest\",\n",
    "            \"run\",\n",
    "            \"@upstash/context7-mcp\",\n",
    "            \"--key\",\n",
    "            \"7c5b4b8f-cb2a-4c2b-b0e0-d3cc49fc7a85\",\n",
    "        ],\n",
    "        \"transport\": \"stdio\",\n",
    "    },\n",
    "}\n",
    "\n",
    "mcp_app = await create_mcp_workflow(server_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await astream_graph(\n",
    "    mcp_app,\n",
    "    inputs={\n",
    "        \"messages\": [\n",
    "            (\n",
    "                \"human\",\n",
    "                \"ìµœì‹  LangGraph ë„íë¨¼íŠ¸ì—ì„œ ReAct Agent ê´€ë ¨ ë‚´ìš©ì„ ê²€ìƒ‰í•˜ì„¸ìš”. ê·¸ëŸ° ë‹¤ìŒ Tavily ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ëŠ” ReAct Agentë¥¼ ìƒì„±í•˜ì„¸ìš”. ì‚¬ìš© LLM=gpt-4.1-mini\",\n",
    "            )\n",
    "        ]\n",
    "    },\n",
    "    config=config,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
