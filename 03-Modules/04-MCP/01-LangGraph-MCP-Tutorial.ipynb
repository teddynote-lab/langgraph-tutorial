{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph MCP (Model Context Protocol) 튜토리얼\n",
    "\n",
    "이 튜토리얼에서는 LangGraph와 MCP(Model Context Protocol)를 통합하여 강력한 AI 에이전트를 구축하는 방법을 배웁니다.\n",
    "\n",
    "## 학습 목표\n",
    "- MCP의 개념과 아키텍처 이해\n",
    "- MultiServerMCPClient를 사용한 다중 서버 관리\n",
    "- React Agent 및 ToolNode와 MCP 통합\n",
    "- 실전 예제를 통한 복잡한 에이전트 구축\n",
    "\n",
    "## 목차\n",
    "1. **MCP 개요 및 설치**\n",
    "2. **기본 MCP 서버 생성**\n",
    "3. **MultiServerMCPClient 설정**\n",
    "4. **React Agent와 MCP 통합**\n",
    "5. **ToolNode와 MCP 통합**\n",
    "6. **다중 MCP 서버 관리**\n",
    "7. **실전 예제 - 복잡한 에이전트 구축**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: MCP 기본 개념\n",
    "\n",
    "### MCP(Model Context Protocol)란?\n",
    "\n",
    "MCP는 애플리케이션이 언어 모델에 도구와 컨텍스트를 제공하는 방법을 표준화한 오픈 프로토콜입니다.\n",
    "\n",
    "#### 주요 특징:\n",
    "- 🔧 **표준화된 도구 인터페이스**: 일관된 방식으로 도구 정의 및 사용\n",
    "- 🌐 **다양한 전송 메커니즘**: stdio, HTTP, WebSocket 등 지원\n",
    "- 🔄 **동적 도구 검색**: 런타임에 도구 자동 검색 및 로드\n",
    "- 🏗️ **확장 가능한 아키텍처**: 여러 서버를 동시에 연결 가능\n",
    "\n",
    "### 설치\n",
    "\n",
    "MCP를 사용하기 위해 필요한 패키지를 설치합니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "# MCP\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경 변수 설정\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# 비동기 호출 활성화\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"LangGraph-MCP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: 기본 MCP 서버 생성\n",
    "\n",
    "MCP 서버는 도구를 제공하는 독립적인 프로세스입니다. FastMCP를 사용하여 간단한 서버를 만들어봅시다.\n",
    "\n",
    "- `server/mcp_server_local.py`\n",
    "- `server/mcp_server_rag.py`\n",
    "- `server/mcp_server_remote.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: MultiServerMCPClient 설정\n",
    "\n",
    "MultiServerMCPClient를 사용하면 여러 MCP 서버를 동시에 관리할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiServerMCPClient 설정 예제\n",
    "async def setup_mcp_client(server_configs: List[Dict[str, Any]]):\n",
    "    \"\"\"MCP 클라이언트를 설정하고 도구를 가져옵니다.\"\"\"\n",
    "\n",
    "    # MCP 클라이언트 생성\n",
    "    client = MultiServerMCPClient(server_configs)\n",
    "\n",
    "    # 도구 가져오기\n",
    "    tools = await client.get_tools()\n",
    "\n",
    "    print(f\"✅ {len(tools)} 개의 MCP 도구가 로드되었습니다:\")\n",
    "    for tool in tools:\n",
    "        print(f\"  - {tool.name}\")\n",
    "\n",
    "    return client, tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 서버 구성 정의\n",
    "server_configs = {\n",
    "    \"weather\": {\n",
    "        \"command\": \"uv\",\n",
    "        \"args\": [\"run\", \"python\", \"server/mcp_server_local.py\"],\n",
    "        \"transport\": \"stdio\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# MCP 클라이언트 생성\n",
    "client, tools = await setup_mcp_client(server_configs=server_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM 설정\n",
    "llm = ChatOpenAI(model=\"gpt-4.1\", temperature=0)\n",
    "\n",
    "# React Agent 생성\n",
    "agent = create_react_agent(\n",
    "    llm, tools, checkpointer=InMemorySaver()  # 상태 저장을 위한 체크포인터\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.messages import astream_graph, random_uuid\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "config = RunnableConfig(configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "response = await astream_graph(\n",
    "    agent,\n",
    "    inputs={\"messages\": [(\"human\", \"안녕하세요. 서울의 날씨를 알려주세요.\")]},\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTTP 전송 방식 사용\n",
    "\n",
    "원격 서버나 HTTP 엔드포인트를 사용하는 경우 먼저 Remote MCP 서버를 구동해야 합니다.\n",
    "\n",
    "```bash\n",
    "uv run python server/mcp_server_remote.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTTP 기반 MCP 서버 설정 예제\n",
    "http_server_config = {\n",
    "    \"current_time\": {\n",
    "        \"url\": \"http://127.0.0.1:8002/mcp\",\n",
    "        \"transport\": \"streamable_http\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# MCP 클라이언트 생성\n",
    "client, http_tools = await setup_mcp_client(server_configs=http_server_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM 설정\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "\n",
    "# React Agent 생성\n",
    "agent = create_react_agent(\n",
    "    llm, http_tools, checkpointer=InMemorySaver()  # 상태 저장을 위한 체크포인터\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.messages import astream_graph, random_uuid\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "config = RunnableConfig(configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "response = await astream_graph(\n",
    "    agent,\n",
    "    inputs={\"messages\": [(\"human\", \"안녕하세요. 현재 시간을 알려주세요.\")]},\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCP Inspector 사용\n",
    "\n",
    "`npx @modelcontextprotocol/inspector` 명령어를 실행하여 브라우저에서 MCP 서버를 테스트할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![mcp_inspector](./assets/mcp-inspector.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTTP 기반 MCP 서버 설정 예제\n",
    "http_server_config = {\n",
    "    \"rag\": {\n",
    "        \"command\": \"uv\",\n",
    "        \"args\": [\"run\", \"python\", \"server/mcp_server_rag.py\"],\n",
    "        \"transport\": \"stdio\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# MCP 클라이언트 생성\n",
    "client, rag_tools = await setup_mcp_client(server_configs=http_server_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM 설정\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "\n",
    "# React Agent 생성\n",
    "rag_agent = create_react_agent(\n",
    "    llm, rag_tools, checkpointer=InMemorySaver()  # 상태 저장을 위한 체크포인터\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.messages import astream_graph, random_uuid\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "config = RunnableConfig(configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "\n",
    "_ = await astream_graph(\n",
    "    rag_agent,\n",
    "    inputs={\n",
    "        \"messages\": [\n",
    "            (\n",
    "                \"human\",\n",
    "                \"삼성전자가 개발한 생성형 AI 의 이름은? mcp 서버를 사용해서 검색해주세요.\",\n",
    "            )\n",
    "        ]\n",
    "    },\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = await astream_graph(\n",
    "    rag_agent,\n",
    "    inputs={\n",
    "        \"messages\": [\n",
    "            (\n",
    "                \"human\",\n",
    "                \"구글이 Anthropic 에 투자하기로 한 금액을 검색해줘\",\n",
    "            )\n",
    "        ]\n",
    "    },\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: React Agent와 MCP 통합\n",
    "\n",
    "React Agent는 추론(Reason)과 행동(Act)을 반복하는 패턴을 구현합니다. MCP 도구와 함께 사용하면 강력한 에이전트를 만들 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def create_mcp_react_agent(server_configs: dict):\n",
    "    \"\"\"MCP 도구를 사용하는 React Agent를 생성합니다.\"\"\"\n",
    "\n",
    "    # MCP 클라이언트 생성 및 도구 가져오기\n",
    "    client, tools = await setup_mcp_client(server_configs=server_configs)\n",
    "\n",
    "    # LLM 설정\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "    # React Agent 생성\n",
    "    agent = create_react_agent(\n",
    "        llm, tools, checkpointer=InMemorySaver()  # 상태 저장을 위한 체크포인터\n",
    "    )\n",
    "\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCP 서버 Config 설정\n",
    "server_configs = {\n",
    "    \"weather\": {\n",
    "        \"command\": \"uv\",\n",
    "        \"args\": [\"run\", \"python\", \"server/mcp_server_local.py\"],\n",
    "        \"transport\": \"stdio\",\n",
    "    },\n",
    "    \"current_time\": {\n",
    "        \"url\": \"http://127.0.0.1:8002/mcp\",\n",
    "        \"transport\": \"streamable_http\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# MCP ReAct Agent 생성\n",
    "agent = await create_mcp_react_agent(server_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_teddynote.messages import random_uuid, astream_graph\n",
    "\n",
    "\n",
    "config = RunnableConfig(configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "await astream_graph(\n",
    "    agent, inputs={\"messages\": [(\"human\", \"현재 시간을 알려주세요\")]}, config=config\n",
    ")\n",
    "\n",
    "await astream_graph(\n",
    "    agent,\n",
    "    inputs={\"messages\": [(\"human\", \"현재 서울의 날씨도 알려주세요\")]},\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: ToolNode와 MCP 통합\n",
    "\n",
    "ToolNode를 사용하면 더 세밀한 제어가 가능한 커스텀 워크플로우를 만들 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langchain_teddynote.graphs import visualize_graph\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage\n",
    "from typing import Annotated, List, Dict, Any\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import TypedDict\n",
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "\n",
    "# 상태 정의\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"에이전트 상태\"\"\"\n",
    "\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    context: Dict[str, Any] = None  # 추가 컨텍스트 정보\n",
    "\n",
    "\n",
    "async def create_mcp_workflow(server_configs: dict):\n",
    "    \"\"\"MCP 도구를 사용하는 커스텀 워크플로우 생성\"\"\"\n",
    "\n",
    "    # MCP 클라이언트 생성\n",
    "    client, tools = await setup_mcp_client(server_configs=server_configs)\n",
    "\n",
    "    # Tavily 검색 도구 추가\n",
    "    tool = TavilySearch(max_results=2)\n",
    "    tools.append(tool)\n",
    "\n",
    "    # LLM 설정 (도구 바인딩)\n",
    "    llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "    llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "    # 워크플로우 그래프 생성\n",
    "    workflow = StateGraph(AgentState)\n",
    "\n",
    "    # 노드 정의\n",
    "    async def agent_node(state: AgentState):\n",
    "        \"\"\"LLM을 호출하여 응답 생성\"\"\"\n",
    "        response = await llm_with_tools.ainvoke(state[\"messages\"])\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    # ToolNode 생성\n",
    "    tool_node = ToolNode(tools)\n",
    "\n",
    "    # 그래프 구성\n",
    "    workflow.add_node(\"agent\", agent_node)\n",
    "    workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "    workflow.add_edge(START, \"agent\")\n",
    "    workflow.add_conditional_edges(\"agent\", tools_condition)\n",
    "    workflow.add_edge(\"tools\", \"agent\")\n",
    "    workflow.add_edge(\"agent\", END)\n",
    "\n",
    "    # 컴파일\n",
    "    app = workflow.compile(checkpointer=InMemorySaver())\n",
    "\n",
    "    visualize_graph(app)\n",
    "\n",
    "    return app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCP 서버 Config 설정\n",
    "server_configs = {\n",
    "    \"weather\": {\n",
    "        \"command\": \"uv\",\n",
    "        \"args\": [\"run\", \"python\", \"server/mcp_server_local.py\"],\n",
    "        \"transport\": \"stdio\",\n",
    "    },\n",
    "    \"current_time\": {\n",
    "        \"url\": \"http://127.0.0.1:8002/mcp\",\n",
    "        \"transport\": \"streamable_http\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcp_app = await create_mcp_workflow(server_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = await astream_graph(\n",
    "    mcp_app, inputs={\"messages\": [(\"human\", \"현재 시간을 알려주세요\")]}, config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = await astream_graph(\n",
    "    mcp_app,\n",
    "    inputs={\n",
    "        \"messages\": [\n",
    "            (\"human\", \"오늘 뉴스를 검색해주세요. 검색시 시간을 조회한 뒤 처리하세요.\")\n",
    "        ]\n",
    "    },\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: 외부 MCP 서버에서 3rd Party 도구 사용하기\n",
    "\n",
    "**Smithery AI란?**\n",
    "\n",
    "- 사이트: https://smithery.ai/\n",
    "\n",
    "- Smithery AI는 AI 에이전트 서비스의 허브 역할을 하는 플랫폼입니다. \n",
    "\n",
    "에이전트형 AI(예: 대형 언어 모델)가 외부 도구나 정보와 효율적으로 연결될 수 있도록 설계된 MCP 서버들을 검색하고 배포하는 역할을 수행합니다. 즉, AI가 다양한 외부 서비스와 손쉽게 통신할 수 있게 하는 중개자이자 생태계 허브입니다.\n",
    "\n",
    "이 모든 서비스의 연결은 **MCP(Model Context Protocol)** 표준 프로토콜을 따릅니다. 이를 통해 AI가 검색, 프로그래밍, 파일 관리, 다양한 API 연동 등 여러 외부 기능을 하나의 허브(플랫폼)만으로 편리하게 활용할 수 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCP 서버 Config 설정\n",
    "server_configs = {\n",
    "    \"weather\": {\n",
    "        \"command\": \"uv\",\n",
    "        \"args\": [\"run\", \"python\", \"server/mcp_server_local.py\"],\n",
    "        \"transport\": \"stdio\",\n",
    "    },\n",
    "    \"current_time\": {\n",
    "        \"url\": \"http://127.0.0.1:8002/mcp\",\n",
    "        \"transport\": \"streamable_http\",\n",
    "    },\n",
    "    \"context7-mcp\": {\n",
    "        \"command\": \"npx\",\n",
    "        \"args\": [\n",
    "            \"-y\",\n",
    "            \"@smithery/cli@latest\",\n",
    "            \"run\",\n",
    "            \"@upstash/context7-mcp\",\n",
    "            \"--key\",\n",
    "            \"7c5b4b8f-cb2a-4c2b-b0e0-d3cc49fc7a85\",\n",
    "        ],\n",
    "        \"transport\": \"stdio\",\n",
    "    },\n",
    "}\n",
    "\n",
    "mcp_app = await create_mcp_workflow(server_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await astream_graph(\n",
    "    mcp_app,\n",
    "    inputs={\n",
    "        \"messages\": [\n",
    "            (\n",
    "                \"human\",\n",
    "                \"최신 LangGraph 도큐먼트에서 ReAct Agent 관련 내용을 검색하세요. 그런 다음 Tavily 검색을 수행하는 ReAct Agent를 생성하세요. 사용 LLM=gpt-4.1-mini\",\n",
    "            )\n",
    "        ]\n",
    "    },\n",
    "    config=config,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
